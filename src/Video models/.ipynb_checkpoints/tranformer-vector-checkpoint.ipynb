{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1536 * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_42\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_76 (InputLayer)       [(None, 16, 1536)]           0         []                            \n",
      "                                                                                                  \n",
      " dense_169 (Dense)           (None, 16, 5092)             7826404   ['input_76[0][0]']            \n",
      "                                                                                                  \n",
      " dense_170 (Dense)           (None, 16, 1536)             7822848   ['dense_169[0][0]']           \n",
      "                                                                                                  \n",
      " multi_head_attention_57 (M  (None, 16, 1536)             155211    ['input_76[0][0]',            \n",
      " ultiHeadAttention)                                                  'input_76[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_51 (Dropout)        (None, 16, 1536)             0         ['dense_170[0][0]']           \n",
      "                                                                                                  \n",
      " layer_normalization_73 (La  (None, 16, 1536)             3072      ['multi_head_attention_57[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " add_44 (Add)                (None, 16, 1536)             0         ['dropout_51[0][0]',          \n",
      "                                                                     'layer_normalization_73[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_74 (La  (None, 16, 1536)             3072      ['add_44[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " flatten_28 (Flatten)        (None, 24576)                0         ['layer_normalization_74[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_171 (Dense)           (None, 1024)                 2516684   ['flatten_28[0][0]']          \n",
      "                                                          8                                       \n",
      "                                                                                                  \n",
      " dense_172 (Dense)           (None, 16)                   16400     ['dense_171[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 40993855 (156.38 MB)\n",
      "Trainable params: 40993855 (156.38 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "import tensorflow as tf\n",
    "\n",
    "inputs = tf.keras.Input(shape=[16, 1536])\n",
    "mh = MultiHeadAttention(num_heads=5, key_dim=5, attention_axes=(2))(inputs, inputs)\n",
    "mhn = tf.keras.layers.LayerNormalization()(mh)\n",
    "\n",
    "d1 = tf.keras.layers.Dense(5092, activation='relu')(inputs)\n",
    "d3 = tf.keras.layers.Dropout(0.3)(l0)\n",
    "d2 = tf.keras.layers.Dense(1536, activation='relu')(d1)\n",
    "d3 = tf.keras.layers.Dropout(0.3)(d2)\n",
    "\n",
    "add = tf.keras.layers.Add()([d3, mhn])\n",
    "\n",
    "l1 = tf.keras.layers.LayerNormalization()(add)\n",
    "f = tf.keras.layers.Flatten()(l1)\n",
    "d4 = tf.keras.layers.Dense(1024, activation='relu')(f)\n",
    "output = tf.keras.layers.Dense(16, activation='softmax')(d4)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "46/46 [==============================] - 2s 14ms/step - loss: 4.9840 - accuracy: 0.3034 - val_loss: 2.6389 - val_accuracy: 0.3505\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.6498 - accuracy: 0.3378 - val_loss: 2.5959 - val_accuracy: 0.3505\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.6081 - accuracy: 0.3378 - val_loss: 2.5556 - val_accuracy: 0.3505\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.5693 - accuracy: 0.3378 - val_loss: 2.5183 - val_accuracy: 0.3505\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.5333 - accuracy: 0.3378 - val_loss: 2.4837 - val_accuracy: 0.3505\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.5001 - accuracy: 0.3378 - val_loss: 2.4520 - val_accuracy: 0.3505\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.4698 - accuracy: 0.3378 - val_loss: 2.4231 - val_accuracy: 0.3505\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.4422 - accuracy: 0.3378 - val_loss: 2.3969 - val_accuracy: 0.3505\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.4172 - accuracy: 0.3378 - val_loss: 2.3732 - val_accuracy: 0.3505\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.3946 - accuracy: 0.3378 - val_loss: 2.3520 - val_accuracy: 0.3505\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.3744 - accuracy: 0.3378 - val_loss: 2.3330 - val_accuracy: 0.3505\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.3563 - accuracy: 0.3378 - val_loss: 2.3161 - val_accuracy: 0.3505\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.3402 - accuracy: 0.3378 - val_loss: 2.3010 - val_accuracy: 0.3505\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.3258 - accuracy: 0.3378 - val_loss: 2.2877 - val_accuracy: 0.3505\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.3131 - accuracy: 0.3378 - val_loss: 2.2760 - val_accuracy: 0.3505\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.3018 - accuracy: 0.3378 - val_loss: 2.2656 - val_accuracy: 0.3505\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2917 - accuracy: 0.3378 - val_loss: 2.2564 - val_accuracy: 0.3505\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2827 - accuracy: 0.3378 - val_loss: 2.2482 - val_accuracy: 0.3505\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2747 - accuracy: 0.3378 - val_loss: 2.2410 - val_accuracy: 0.3505\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2676 - accuracy: 0.3378 - val_loss: 2.2347 - val_accuracy: 0.3505\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2612 - accuracy: 0.3378 - val_loss: 2.2290 - val_accuracy: 0.3505\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2555 - accuracy: 0.3378 - val_loss: 2.2239 - val_accuracy: 0.3505\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2503 - accuracy: 0.3378 - val_loss: 2.2194 - val_accuracy: 0.3505\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2456 - accuracy: 0.3378 - val_loss: 2.2154 - val_accuracy: 0.3505\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2414 - accuracy: 0.3378 - val_loss: 2.2117 - val_accuracy: 0.3505\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2375 - accuracy: 0.3378 - val_loss: 2.2084 - val_accuracy: 0.3505\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2340 - accuracy: 0.3378 - val_loss: 2.2055 - val_accuracy: 0.3505\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2307 - accuracy: 0.3378 - val_loss: 2.2027 - val_accuracy: 0.3505\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2277 - accuracy: 0.3378 - val_loss: 2.2003 - val_accuracy: 0.3505\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2249 - accuracy: 0.3378 - val_loss: 2.1980 - val_accuracy: 0.3505\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2224 - accuracy: 0.3378 - val_loss: 2.1959 - val_accuracy: 0.3505\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2200 - accuracy: 0.3378 - val_loss: 2.1940 - val_accuracy: 0.3505\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2178 - accuracy: 0.3378 - val_loss: 2.1923 - val_accuracy: 0.3505\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2157 - accuracy: 0.3378 - val_loss: 2.1906 - val_accuracy: 0.3505\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.2137 - accuracy: 0.3378 - val_loss: 2.1891 - val_accuracy: 0.3505\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.2119 - accuracy: 0.3378 - val_loss: 2.1877 - val_accuracy: 0.3505\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2102 - accuracy: 0.3378 - val_loss: 2.1864 - val_accuracy: 0.3505\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.2085 - accuracy: 0.3378 - val_loss: 2.1852 - val_accuracy: 0.3505\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2070 - accuracy: 0.3378 - val_loss: 2.1840 - val_accuracy: 0.3505\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2056 - accuracy: 0.3378 - val_loss: 2.1829 - val_accuracy: 0.3505\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.2042 - accuracy: 0.3378 - val_loss: 2.1819 - val_accuracy: 0.3505\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2029 - accuracy: 0.3378 - val_loss: 2.1809 - val_accuracy: 0.3505\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2017 - accuracy: 0.3378 - val_loss: 2.1800 - val_accuracy: 0.3505\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2005 - accuracy: 0.3378 - val_loss: 2.1791 - val_accuracy: 0.3505\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1994 - accuracy: 0.3378 - val_loss: 2.1783 - val_accuracy: 0.3505\n",
      "Epoch 46/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1983 - accuracy: 0.3378 - val_loss: 2.1775 - val_accuracy: 0.3505\n",
      "Epoch 47/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1973 - accuracy: 0.3378 - val_loss: 2.1768 - val_accuracy: 0.3505\n",
      "Epoch 48/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1963 - accuracy: 0.3378 - val_loss: 2.1761 - val_accuracy: 0.3505\n",
      "Epoch 49/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1954 - accuracy: 0.3378 - val_loss: 2.1754 - val_accuracy: 0.3505\n",
      "Epoch 50/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1945 - accuracy: 0.3378 - val_loss: 2.1747 - val_accuracy: 0.3505\n",
      "Epoch 51/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1937 - accuracy: 0.3378 - val_loss: 2.1741 - val_accuracy: 0.3505\n",
      "Epoch 52/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1929 - accuracy: 0.3378 - val_loss: 2.1735 - val_accuracy: 0.3505\n",
      "Epoch 53/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1921 - accuracy: 0.3378 - val_loss: 2.1730 - val_accuracy: 0.3505\n",
      "Epoch 54/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1914 - accuracy: 0.3378 - val_loss: 2.1724 - val_accuracy: 0.3505\n",
      "Epoch 55/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1907 - accuracy: 0.3378 - val_loss: 2.1719 - val_accuracy: 0.3505\n",
      "Epoch 56/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1900 - accuracy: 0.3378 - val_loss: 2.1714 - val_accuracy: 0.3505\n",
      "Epoch 57/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1893 - accuracy: 0.3378 - val_loss: 2.1709 - val_accuracy: 0.3505\n",
      "Epoch 58/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1887 - accuracy: 0.3378 - val_loss: 2.1704 - val_accuracy: 0.3505\n",
      "Epoch 59/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1881 - accuracy: 0.3378 - val_loss: 2.1699 - val_accuracy: 0.3505\n",
      "Epoch 60/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1875 - accuracy: 0.3378 - val_loss: 2.1695 - val_accuracy: 0.3505\n",
      "Epoch 61/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1869 - accuracy: 0.3378 - val_loss: 2.1691 - val_accuracy: 0.3505\n",
      "Epoch 62/100\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.1864 - accuracy: 0.3378 - val_loss: 2.1687 - val_accuracy: 0.3505\n",
      "Epoch 63/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1859 - accuracy: 0.3378 - val_loss: 2.1683 - val_accuracy: 0.3505\n",
      "Epoch 64/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1854 - accuracy: 0.3378 - val_loss: 2.1679 - val_accuracy: 0.3505\n",
      "Epoch 65/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1848 - accuracy: 0.3378 - val_loss: 2.1675 - val_accuracy: 0.3505\n",
      "Epoch 66/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1844 - accuracy: 0.3378 - val_loss: 2.1672 - val_accuracy: 0.3505\n",
      "Epoch 67/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1839 - accuracy: 0.3378 - val_loss: 2.1668 - val_accuracy: 0.3505\n",
      "Epoch 68/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1835 - accuracy: 0.3378 - val_loss: 2.1664 - val_accuracy: 0.3505\n",
      "Epoch 69/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1831 - accuracy: 0.3378 - val_loss: 2.1661 - val_accuracy: 0.3505\n",
      "Epoch 70/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1826 - accuracy: 0.3378 - val_loss: 2.1658 - val_accuracy: 0.3505\n",
      "Epoch 71/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1822 - accuracy: 0.3378 - val_loss: 2.1654 - val_accuracy: 0.3505\n",
      "Epoch 72/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1818 - accuracy: 0.3378 - val_loss: 2.1651 - val_accuracy: 0.3505\n",
      "Epoch 73/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1815 - accuracy: 0.3378 - val_loss: 2.1648 - val_accuracy: 0.3505\n",
      "Epoch 74/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1811 - accuracy: 0.3378 - val_loss: 2.1645 - val_accuracy: 0.3505\n",
      "Epoch 75/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1807 - accuracy: 0.3378 - val_loss: 2.1642 - val_accuracy: 0.3505\n",
      "Epoch 76/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1804 - accuracy: 0.3378 - val_loss: 2.1639 - val_accuracy: 0.3505\n",
      "Epoch 77/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1801 - accuracy: 0.3378 - val_loss: 2.1637 - val_accuracy: 0.3505\n",
      "Epoch 78/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1797 - accuracy: 0.3378 - val_loss: 2.1634 - val_accuracy: 0.3505\n",
      "Epoch 79/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1794 - accuracy: 0.3378 - val_loss: 2.1631 - val_accuracy: 0.3505\n",
      "Epoch 80/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1791 - accuracy: 0.3378 - val_loss: 2.1629 - val_accuracy: 0.3505\n",
      "Epoch 81/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1788 - accuracy: 0.3378 - val_loss: 2.1626 - val_accuracy: 0.3505\n",
      "Epoch 82/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1785 - accuracy: 0.3378 - val_loss: 2.1624 - val_accuracy: 0.3505\n",
      "Epoch 83/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1783 - accuracy: 0.3378 - val_loss: 2.1621 - val_accuracy: 0.3505\n",
      "Epoch 84/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1780 - accuracy: 0.3378 - val_loss: 2.1619 - val_accuracy: 0.3505\n",
      "Epoch 85/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1777 - accuracy: 0.3378 - val_loss: 2.1617 - val_accuracy: 0.3505\n",
      "Epoch 86/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1775 - accuracy: 0.3378 - val_loss: 2.1614 - val_accuracy: 0.3505\n",
      "Epoch 87/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1772 - accuracy: 0.3378 - val_loss: 2.1612 - val_accuracy: 0.3505\n",
      "Epoch 88/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1769 - accuracy: 0.3378 - val_loss: 2.1610 - val_accuracy: 0.3505\n",
      "Epoch 89/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1767 - accuracy: 0.3378 - val_loss: 2.1608 - val_accuracy: 0.3505\n",
      "Epoch 90/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1765 - accuracy: 0.3378 - val_loss: 2.1606 - val_accuracy: 0.3505\n",
      "Epoch 91/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1762 - accuracy: 0.3378 - val_loss: 2.1604 - val_accuracy: 0.3505\n",
      "Epoch 92/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1760 - accuracy: 0.3378 - val_loss: 2.1602 - val_accuracy: 0.3505\n",
      "Epoch 93/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1758 - accuracy: 0.3378 - val_loss: 2.1600 - val_accuracy: 0.3505\n",
      "Epoch 94/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1756 - accuracy: 0.3378 - val_loss: 2.1598 - val_accuracy: 0.3505\n",
      "Epoch 95/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1754 - accuracy: 0.3378 - val_loss: 2.1596 - val_accuracy: 0.3505\n",
      "Epoch 96/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1752 - accuracy: 0.3378 - val_loss: 2.1594 - val_accuracy: 0.3505\n",
      "Epoch 97/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1750 - accuracy: 0.3378 - val_loss: 2.1593 - val_accuracy: 0.3505\n",
      "Epoch 98/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1748 - accuracy: 0.3378 - val_loss: 2.1591 - val_accuracy: 0.3505\n",
      "Epoch 99/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1746 - accuracy: 0.3378 - val_loss: 2.1589 - val_accuracy: 0.3505\n",
      "Epoch 100/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1744 - accuracy: 0.3378 - val_loss: 2.1588 - val_accuracy: 0.3505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb575489460>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "            epochs=100,\n",
    "            batch_size=64, \n",
    "            validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 2ms/step - loss: 2.2466 - accuracy: 0.2859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2466201782226562, 0.2858695685863495]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative modele\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.01, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "input_tensor = tf.keras.Input(shape=[16, 1536])\n",
    "layer = MultiHeadAttention(num_heads=5, key_dim=5, attention_axes=(1, 2))\n",
    "x = layer(input_tensor, input_tensor)\n",
    "layer_norm = tf.keras.layers.LayerNormalization()\n",
    "x = layer_norm(x)\n",
    "\n",
    "d_model = 1536\n",
    "\n",
    "seq = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(2048, activation='relu'),\n",
    "    tf.keras.layers.Dense(d_model, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1)])\n",
    "\n",
    "add = tf.keras.layers.Add()\n",
    "x = add([x, seq(x)])\n",
    "layer_norm = tf.keras.layers.LayerNormalization()\n",
    "x = layer_norm(x)\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "final = tf.keras.layers.Dense(1536, activation='tanh')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_tensor, outputs=final)\n",
    "model.compile(loss='cosine_similarity', optimizer='adam', metrics=['cosine_similarity'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "            epochs=1000,\n",
    "            batch_size=64, \n",
    "            validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_54 (InputLayer)       [(None, 16, 128)]            0         []                            \n",
      "                                                                                                  \n",
      " multi_head_attention_42 (M  (None, 16, 128)              4763      ['input_54[0][0]',            \n",
      " ultiHeadAttention)                                                  'input_54[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_38 (La  (None, 16, 128)              256       ['multi_head_attention_42[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " sequential_27 (Sequential)  (None, 16, 128)              526464    ['layer_normalization_38[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_27 (Add)                (None, 16, 128)              0         ['layer_normalization_38[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'sequential_27[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_39 (La  (None, 16, 128)              256       ['add_27[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " flatten_14 (Flatten)        (None, 2048)                 0         ['layer_normalization_39[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_94 (Dense)            (None, 128)                  262272    ['flatten_14[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 794011 (3.03 MB)\n",
      "Trainable params: 794011 (3.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/111 [..............................] - ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 0s 2ms/step\n",
      "0.32\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tp = 0\n",
    "fp = 0\n",
    "\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "for i in range(100):\n",
    "    p = np.argmax(cosine_similarity(X_test[i], [pred[i]])) == np.argmax(cosine_similarity(X_test[i], [y_test[i]]))\n",
    "    if p:\n",
    "        tp += 1\n",
    "    fp += 1\n",
    "\n",
    "print(tp/fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "\n",
    "\n",
    "df = pd.read_pickle('../../data/datasets/raw-video-level-watches')\n",
    "videos = pd.read_pickle('../../data/videos_raw_metadata')\n",
    "title_embeddings = pd.read_pickle('../../data/embeddings/openai-title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "playing = watch['playing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01953424, -0.01826318,  0.0135201 , ...,  0.00290338,\n",
       "       -0.00709789,  0.00755279])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(title_embeddings[videos[playing]['snippet']['title']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ca7ed9ad41451da70624b183043025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Categorical\n",
    "\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for watch in tqdm(df):\n",
    "    playing = watch['playing']\n",
    "    selected = watch['selected']\n",
    "    if playing not in videos or selected not in videos or any([s not in videos for s in watch['upnext']]):\n",
    "        continue\n",
    "\n",
    "    if len(watch['upnext']) < 15:\n",
    "        continue\n",
    "        \n",
    "    upnext = watch['upnext'][:15]\n",
    "\n",
    "    playing = watch['playing']\n",
    "    playing = np.array(title_embeddings[videos[playing]['snippet']['title']])\n",
    "    all_suggested = [np.array(title_embeddings[videos[s]['snippet']['title']]) for s in upnext]\n",
    "\n",
    "    selected = watch['selected']\n",
    "    y = [0]\n",
    "    for s in upnext:\n",
    "        if s == selected:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "\n",
    "    x = np.vstack([playing, all_suggested])\n",
    "    X.append(x)\n",
    "    Y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4599, 16)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4599, 16, 1536)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71721c5f5fba4628ba23d1a5f7d40f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generative\n",
    "\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for watch in tqdm(df):\n",
    "    playing = watch['playing']\n",
    "    selected = watch['selected']\n",
    "    if playing not in videos or selected not in videos or any([s not in videos for s in watch['upnext']]):\n",
    "        continue\n",
    "\n",
    "    if len(watch['upnext']) < 15:\n",
    "        continue\n",
    "        \n",
    "    upnext = watch['upnext'][:15]\n",
    "\n",
    "    playing = watch['playing']\n",
    "    playing = np.array(title_embeddings[videos[playing]['snippet']['title']])\n",
    "    all_suggested = [np.array(title_embeddings[videos[s]['snippet']['title']]) for s in upnext]\n",
    "\n",
    "    selected = watch['selected']\n",
    "    y = []\n",
    "    for s in upnext:\n",
    "        if s == selected:\n",
    "            y = np.array(title_embeddings[videos[selected]['snippet']['title']])\n",
    "\n",
    "    if len(y) == 0:\n",
    "        continue\n",
    "\n",
    "    x = np.vstack([playing, all_suggested])\n",
    "    X.append(x)\n",
    "    Y.append(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4413, 16, 1536)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26caa140dcc24604b0deb6fb09df914c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generative seperate\n",
    "\n",
    "pX = []\n",
    "sX = []\n",
    "Y = []\n",
    "for watch in tqdm(df):\n",
    "    playing = watch['playing']\n",
    "    selected = watch['selected']\n",
    "    if playing not in videos or selected not in videos or any([s not in videos for s in watch['upnext']]):\n",
    "        continue\n",
    "\n",
    "    if len(watch['upnext']) < 15:\n",
    "        continue\n",
    "        \n",
    "    upnext = watch['upnext'][:15]\n",
    "\n",
    "    playing = watch['playing']\n",
    "    playing = np.array(title_embeddings[videos[playing]['snippet']['title']])\n",
    "    all_suggested = [np.array(title_embeddings[videos[s]['snippet']['title']]) for s in upnext]\n",
    "\n",
    "    selected = watch['selected']\n",
    "    y = []\n",
    "    for s in upnext:\n",
    "        if s == selected:\n",
    "            y = np.array(title_embeddings[videos[selected]['snippet']['title']])\n",
    "\n",
    "    if len(y) == 0:\n",
    "        continue\n",
    "\n",
    "    pX.append(playing)\n",
    "    sX.append(all_suggested)\n",
    "    Y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4413, 15, 1536)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sX = np.array(sX)\n",
    "pX = np.array(pX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4413, 1, 1536)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dbd0840a53844d6bd3355959f3bc330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generative seperate\n",
    "\n",
    "pX = []\n",
    "sX = []\n",
    "isX = []\n",
    "Y = []\n",
    "for watch in tqdm(df):\n",
    "    playing = watch['playing']\n",
    "    selected = watch['selected']\n",
    "    if playing not in videos or selected not in videos or any([s not in videos for s in watch['upnext']]):\n",
    "        continue\n",
    "\n",
    "    if len(watch['upnext']) < 15:\n",
    "        continue\n",
    "        \n",
    "    upnext = watch['upnext'][:15]\n",
    "\n",
    "    playing = watch['playing']\n",
    "    playing = np.array(title_embeddings[videos[playing]['snippet']['title']])\n",
    "    all_suggested = [np.array(title_embeddings[videos[s]['snippet']['title']]) for s in upnext]\n",
    "\n",
    "    selected = watch['selected']\n",
    "    y = []\n",
    "    for s in upnext:\n",
    "        if s == selected:\n",
    "            y = 1\n",
    "        else:\n",
    "            y = 0\n",
    "\n",
    "        s = np.array(title_embeddings[videos[s]['snippet']['title']])\n",
    "\n",
    "        pX.append(playing)\n",
    "        sX.append(all_suggested)\n",
    "        isX.append(s)\n",
    "        Y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(Y)\n",
    "pX = np.array(pX)\n",
    "pX = pX.reshape(pX.shape[0], 1, pX.shape[1])\n",
    "isX = np.array(isX)\n",
    "isX = isX.reshape(isX.shape[0], 1, isX.shape[1])\n",
    "sX = np.array(sX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pX_train, pX_test, isX_train, isX_test, sX_train, sX_test, y_train, y_test = train_test_split(pX, isX, sX, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55188, 1, 1536),\n",
       " (13797, 1, 1536),\n",
       " (55188, 1, 1536),\n",
       " (13797, 1, 1536),\n",
       " (55188, 15, 1536),\n",
       " (13797, 15, 1536),\n",
       " (55188,),\n",
       " (13797,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pX_train.shape, pX_test.shape, isX_train.shape, isX_test.shape, sX_train.shape, sX_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 1920])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102.4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_24 (InputLayer)       [(None, 15, 1536)]           0         []                            \n",
      "                                                                                                  \n",
      " dense_33 (Dense)            (None, 15, 512)              786944    ['input_24[0][0]']            \n",
      "                                                                                                  \n",
      " input_25 (InputLayer)       [(None, 1, 1536)]            0         []                            \n",
      "                                                                                                  \n",
      " dense_34 (Dense)            (None, 15, 64)               32832     ['dense_33[1][0]']            \n",
      "                                                                                                  \n",
      " dense_35 (Dense)            (None, 1, 960)               1475520   ['input_25[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_11 (Flatten)        (None, 960)                  0         ['dense_34[1][0]']            \n",
      "                                                                                                  \n",
      " flatten_12 (Flatten)        (None, 960)                  0         ['dense_35[1][0]']            \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate  (None, 1920)                 0         ['flatten_11[1][0]',          \n",
      " )                                                                   'flatten_12[1][0]']          \n",
      "                                                                                                  \n",
      " input_27 (InputLayer)       [(None, 1536)]               0         []                            \n",
      "                                                                                                  \n",
      " dense_36 (Dense)            (None, 512)                  983552    ['concatenate_5[1][0]']       \n",
      "                                                                                                  \n",
      " dense_37 (Dense)            (None, 512)                  786944    ['input_27[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate  (None, 1024)                 0         ['dense_36[1][0]',            \n",
      " )                                                                   'dense_37[1][0]']            \n",
      "                                                                                                  \n",
      " flatten_14 (Flatten)        (None, 1024)                 0         ['concatenate_6[1][0]']       \n",
      "                                                                                                  \n",
      " dense_38 (Dense)            (None, 512)                  524800    ['flatten_14[1][0]']          \n",
      "                                                                                                  \n",
      " dense_39 (Dense)            (None, 1)                    513       ['dense_38[1][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4591105 (17.51 MB)\n",
      "Trainable params: 4591105 (17.51 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# binary separate inputs\n",
    "\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "suggested_tensor = tf.keras.Input(shape=[15, 1536])\n",
    "playing_tensor = tf.keras.Input(shape=[1, 1536])\n",
    "# layer = MultiHeadAttention(num_heads=15, key_dim=15, attention_axes=(1), name='attention')\n",
    "# # x = layer(playing_tensor, suggested_tensor)\n",
    "# x = layer(suggested_tensor, playing_tensor)\n",
    "# x = layer(suggested_tensor, suggested_tensor)\n",
    "# layer_norm = tf.keras.layers.LayerNormalization()\n",
    "# x = layer_norm(x)\n",
    "\n",
    "st_dense = tf.keras.layers.Dense(512, activation='relu')(suggested_tensor)\n",
    "st_dense = tf.keras.layers.Dense(64, activation='relu')(st_dense)\n",
    "st = tf.keras.layers.Flatten()(st_dense)\n",
    "\n",
    "pt = tf.keras.layers.Dense(960, activation='relu')(playing_tensor)\n",
    "pt = tf.keras.layers.Flatten()(pt)\n",
    "\n",
    "merged = tf.keras.layers.Concatenate()([st, pt])\n",
    "mem = tf.keras.layers.Dense(512, activation='relu')(merged)\n",
    "\n",
    "is_tensor = tf.keras.Input(shape=[1, 1536])\n",
    "is_t = tf.keras.layers.Flatten()(is_t)\n",
    "is_t = tf.keras.layers.Dense(512, activation='relu')(is_t)\n",
    "\n",
    "merged = tf.keras.layers.Concatenate()([mem, is_t])\n",
    "\n",
    "x = tf.keras.layers.Flatten()(merged)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "final = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[suggested_tensor, playing_tensor, is_tensor], outputs=final)\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='acc'),\n",
    "      keras.metrics.Precision(name='P'),\n",
    "      keras.metrics.Recall(name='R'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=METRICS)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.534742570113018, 1: 7.69578313253012}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = Y.sum()\n",
    "neg = len(Y) - pos\n",
    "total = neg + pos\n",
    "w0 = (1 / neg) * (total / 2.0)\n",
    "w1 = (1 / pos) * (total / 2.0)\n",
    "class_weights = {0:w0, 1:w1}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/hsmhabib/.conda/envs/ytbase/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/hsmhabib/.conda/envs/ytbase/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/hsmhabib/.conda/envs/ytbase/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/hsmhabib/.conda/envs/ytbase/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/hsmhabib/.conda/envs/ytbase/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/hsmhabib/.conda/envs/ytbase/lib/python3.9/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 2 of layer \"model_2\" is incompatible with the layer: expected shape=(None, 1536), found shape=(None, 1, 1536)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misX_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file3efmgkue.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/hsmhabib/.conda/envs/ytbase/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/hsmhabib/.conda/envs/ytbase/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/hsmhabib/.conda/envs/ytbase/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/hsmhabib/.conda/envs/ytbase/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/hsmhabib/.conda/envs/ytbase/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/hsmhabib/.conda/envs/ytbase/lib/python3.9/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 2 of layer \"model_2\" is incompatible with the layer: expected shape=(None, 1536), found shape=(None, 1, 1536)\n"
     ]
    }
   ],
   "source": [
    "model.fit([sX_train, pX_train, isX_train], y_train, epochs=1000, batch_size=64, validation_split=0.1, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_64\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_124 (InputLayer)      [(None, 1, 1536)]            0         []                            \n",
      "                                                                                                  \n",
      " input_123 (InputLayer)      [(None, 15, 1536)]           0         []                            \n",
      "                                                                                                  \n",
      " attention (MultiHeadAttent  (None, 1, 1536)              1384611   ['input_124[0][0]',           \n",
      " ion)                                                                'input_123[0][0]']           \n",
      "                                                                                                  \n",
      " layer_normalization_114 (L  (None, 1, 1536)              3072      ['attention[0][0]']           \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " sequential_63 (Sequential)  (None, 1, 1536)              6295040   ['layer_normalization_114[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_64 (Add)                (None, 1, 1536)              0         ['layer_normalization_114[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'sequential_63[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_115 (L  (None, 1, 1536)              3072      ['add_64[0][0]']              \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " flatten_48 (Flatten)        (None, 1536)                 0         ['layer_normalization_115[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dense_235 (Dense)           (None, 1536)                 2360832   ['flatten_48[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10046627 (38.32 MB)\n",
      "Trainable params: 10046627 (38.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# generative separate inputs\n",
    "\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "suggested_tensor = tf.keras.Input(shape=[15, 1536])\n",
    "playing_tensor = tf.keras.Input(shape=[1, 1536])\n",
    "layer = MultiHeadAttention(num_heads=15, key_dim=15, attention_axes=(1), name='attention')\n",
    "x = layer(playing_tensor, suggested_tensor)\n",
    "# x = layer(suggested_tensor, suggested_tensor)\n",
    "layer_norm = tf.keras.layers.LayerNormalization()\n",
    "x = layer_norm(x)\n",
    "\n",
    "d_model = 1536\n",
    "\n",
    "seq = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(2048, activation='relu'),\n",
    "    tf.keras.layers.Dense(d_model, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1)\n",
    "])\n",
    "add = tf.keras.layers.Add()\n",
    "x = add([x, seq(x)])\n",
    "layer_norm = tf.keras.layers.LayerNormalization()\n",
    "x = layer_norm(x)\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "final = tf.keras.layers.Dense(1536, activation='tanh')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[suggested_tensor, playing_tensor], outputs=final)\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['cosine_similarity'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query (1536, 15, 15)\n",
      "keys (15, 15)\n",
      "values (1536, 15, 15)\n",
      "proj (15, 15)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = model.get_layer('attention')\n",
    "weight_names = ['query', 'keys',  'values', 'proj']\n",
    "for name, out in zip(weight_names,layer.get_weights()):\n",
    "    print(name, out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = np.array(Y)\n",
    "# pX = pX.reshape((pX.shape[0], 1,pX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit([sX_train, pX_train], y_train, epochs=1000, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 14ms/step - loss: 3.1438e-04 - cosine_similarity: 0.7496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.00031437957659363747, 0.7495620846748352]"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([sX_test, pX_test], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.argsort([1, 2, 3, 4, 99, 6, 44, 0])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6, 5, 3, 2, 1, 0, 7])"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  5,  9])"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tp = 0\n",
    "fp = 0\n",
    "\n",
    "\n",
    "pred = model.predict([sX_test, pX_test])\n",
    "\n",
    "I = []\n",
    "J = []\n",
    "\n",
    "for ii in range(0, 17):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for i in range(100):\n",
    "        p = np.argsort(np.array(cosine_similarity(sX_test[i], [pred[i]])).reshape(15))[::-1]\n",
    "        p = p[:ii]\n",
    "        y = np.argmax(cosine_similarity(sX_test[i], [y_test[i]]))\n",
    "        if y in p:\n",
    "            tp += 1\n",
    "        fp += 1\n",
    "\n",
    "    I.append(ii)\n",
    "    J.append(tp/fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y in p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.09,\n",
       " 0.17,\n",
       " 0.21,\n",
       " 0.32,\n",
       " 0.36,\n",
       " 0.42,\n",
       " 0.48,\n",
       " 0.56,\n",
       " 0.61,\n",
       " 0.7,\n",
       " 0.79,\n",
       " 0.84,\n",
       " 0.92,\n",
       " 0.93,\n",
       " 1.0,\n",
       " 1.0]"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fb4a4b83460>"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(I, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_54\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_104 (InputLayer)      [(None, 1, 1536)]            0         []                            \n",
      "                                                                                                  \n",
      " input_103 (InputLayer)      [(None, 15, 1536)]           0         []                            \n",
      "                                                                                                  \n",
      " multi_head_attention_71 (M  (None, 1, 1536)              155211    ['input_104[0][0]',           \n",
      " ultiHeadAttention)                                                  'input_103[0][0]']           \n",
      "                                                                                                  \n",
      " layer_normalization_98 (La  (None, 1, 1536)              3072      ['multi_head_attention_71[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " sequential_55 (Sequential)  (None, 1, 1536)              6295040   ['layer_normalization_98[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_56 (Add)                (None, 1, 1536)              0         ['layer_normalization_98[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'sequential_55[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_99 (La  (None, 1, 1536)              3072      ['add_56[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " flatten_40 (Flatten)        (None, 1536)                 0         ['layer_normalization_99[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_210 (Dense)           (None, 1536)                 2360832   ['flatten_40[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8817227 (33.64 MB)\n",
      "Trainable params: 8817227 (33.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query (1536, 15, 1)\n",
      "keys (15, 1)\n",
      "values (1536, 15, 1)\n",
      "proj (15, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = model.get_layer('attention')\n",
    "weight_names = ['query', 'keys',  'values', 'proj']\n",
    "for name, out in zip(weight_names,layer.get_weights()):\n",
    "    print(name, out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = tf.keras.Model(inputs=model.input, \n",
    "                                 outputs=model.get_layer(\"attention\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/138 [..............................] - ETA: 3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsmhabib/.conda/envs/ytbase/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "yy = attention.predict([sX, pX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4413, 1, 1536)"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "suggested_tensor = tf.keras.Input(shape=[15, 1536])\n",
    "playing_tensor = tf.keras.Input(shape=[1, 1536])\n",
    "layer = MultiHeadAttention(num_heads=5, key_dim=5)\n",
    "x = layer(suggested_tensor, playing_tensor)\n",
    "layer_norm = tf.keras.layers.LayerNormalization()\n",
    "x = layer_norm(x)\n",
    "\n",
    "d_model = 1536\n",
    "\n",
    "seq = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(2048, activation='relu'),\n",
    "    tf.keras.layers.Dense(d_model, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1)\n",
    "])\n",
    "add = tf.keras.layers.Add()\n",
    "x = add([x, seq(x)])\n",
    "layer_norm = tf.keras.layers.LayerNormalization()\n",
    "x = layer_norm(x)\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "final = tf.keras.layers.Dense(1536, activation='relu')(x)\n",
    "final = tf.keras.layers.Dense(1536, activation='relu')(x)\n",
    "final = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "final = tf.keras.layers.Dense(15, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[suggested_tensor, playing_tensor], outputs=final)\n",
    "model.compile(loss='cosine_similarity', optimizer='adam', metrics=['cosine_similarity'])\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
