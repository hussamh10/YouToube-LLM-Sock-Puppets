{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1536 * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_42\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_76 (InputLayer)       [(None, 16, 1536)]           0         []                            \n",
      "                                                                                                  \n",
      " dense_169 (Dense)           (None, 16, 5092)             7826404   ['input_76[0][0]']            \n",
      "                                                                                                  \n",
      " dense_170 (Dense)           (None, 16, 1536)             7822848   ['dense_169[0][0]']           \n",
      "                                                                                                  \n",
      " multi_head_attention_57 (M  (None, 16, 1536)             155211    ['input_76[0][0]',            \n",
      " ultiHeadAttention)                                                  'input_76[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_51 (Dropout)        (None, 16, 1536)             0         ['dense_170[0][0]']           \n",
      "                                                                                                  \n",
      " layer_normalization_73 (La  (None, 16, 1536)             3072      ['multi_head_attention_57[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " add_44 (Add)                (None, 16, 1536)             0         ['dropout_51[0][0]',          \n",
      "                                                                     'layer_normalization_73[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_74 (La  (None, 16, 1536)             3072      ['add_44[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " flatten_28 (Flatten)        (None, 24576)                0         ['layer_normalization_74[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_171 (Dense)           (None, 1024)                 2516684   ['flatten_28[0][0]']          \n",
      "                                                          8                                       \n",
      "                                                                                                  \n",
      " dense_172 (Dense)           (None, 16)                   16400     ['dense_171[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 40993855 (156.38 MB)\n",
      "Trainable params: 40993855 (156.38 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "import tensorflow as tf\n",
    "\n",
    "inputs = tf.keras.Input(shape=[16, 1536])\n",
    "mh = MultiHeadAttention(num_heads=5, key_dim=5, attention_axes=(2))(inputs, inputs)\n",
    "mhn = tf.keras.layers.LayerNormalization()(mh)\n",
    "\n",
    "d1 = tf.keras.layers.Dense(5092, activation='relu')(inputs)\n",
    "d3 = tf.keras.layers.Dropout(0.3)(l0)\n",
    "d2 = tf.keras.layers.Dense(1536, activation='relu')(d1)\n",
    "d3 = tf.keras.layers.Dropout(0.3)(d2)\n",
    "\n",
    "add = tf.keras.layers.Add()([d3, mhn])\n",
    "\n",
    "l1 = tf.keras.layers.LayerNormalization()(add)\n",
    "f = tf.keras.layers.Flatten()(l1)\n",
    "d4 = tf.keras.layers.Dense(1024, activation='relu')(f)\n",
    "output = tf.keras.layers.Dense(16, activation='softmax')(d4)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "46/46 [==============================] - 2s 14ms/step - loss: 4.9840 - accuracy: 0.3034 - val_loss: 2.6389 - val_accuracy: 0.3505\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.6498 - accuracy: 0.3378 - val_loss: 2.5959 - val_accuracy: 0.3505\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.6081 - accuracy: 0.3378 - val_loss: 2.5556 - val_accuracy: 0.3505\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.5693 - accuracy: 0.3378 - val_loss: 2.5183 - val_accuracy: 0.3505\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.5333 - accuracy: 0.3378 - val_loss: 2.4837 - val_accuracy: 0.3505\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.5001 - accuracy: 0.3378 - val_loss: 2.4520 - val_accuracy: 0.3505\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.4698 - accuracy: 0.3378 - val_loss: 2.4231 - val_accuracy: 0.3505\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.4422 - accuracy: 0.3378 - val_loss: 2.3969 - val_accuracy: 0.3505\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.4172 - accuracy: 0.3378 - val_loss: 2.3732 - val_accuracy: 0.3505\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.3946 - accuracy: 0.3378 - val_loss: 2.3520 - val_accuracy: 0.3505\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.3744 - accuracy: 0.3378 - val_loss: 2.3330 - val_accuracy: 0.3505\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.3563 - accuracy: 0.3378 - val_loss: 2.3161 - val_accuracy: 0.3505\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.3402 - accuracy: 0.3378 - val_loss: 2.3010 - val_accuracy: 0.3505\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.3258 - accuracy: 0.3378 - val_loss: 2.2877 - val_accuracy: 0.3505\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.3131 - accuracy: 0.3378 - val_loss: 2.2760 - val_accuracy: 0.3505\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.3018 - accuracy: 0.3378 - val_loss: 2.2656 - val_accuracy: 0.3505\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2917 - accuracy: 0.3378 - val_loss: 2.2564 - val_accuracy: 0.3505\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2827 - accuracy: 0.3378 - val_loss: 2.2482 - val_accuracy: 0.3505\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2747 - accuracy: 0.3378 - val_loss: 2.2410 - val_accuracy: 0.3505\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2676 - accuracy: 0.3378 - val_loss: 2.2347 - val_accuracy: 0.3505\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2612 - accuracy: 0.3378 - val_loss: 2.2290 - val_accuracy: 0.3505\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2555 - accuracy: 0.3378 - val_loss: 2.2239 - val_accuracy: 0.3505\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2503 - accuracy: 0.3378 - val_loss: 2.2194 - val_accuracy: 0.3505\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2456 - accuracy: 0.3378 - val_loss: 2.2154 - val_accuracy: 0.3505\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2414 - accuracy: 0.3378 - val_loss: 2.2117 - val_accuracy: 0.3505\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2375 - accuracy: 0.3378 - val_loss: 2.2084 - val_accuracy: 0.3505\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2340 - accuracy: 0.3378 - val_loss: 2.2055 - val_accuracy: 0.3505\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2307 - accuracy: 0.3378 - val_loss: 2.2027 - val_accuracy: 0.3505\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2277 - accuracy: 0.3378 - val_loss: 2.2003 - val_accuracy: 0.3505\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2249 - accuracy: 0.3378 - val_loss: 2.1980 - val_accuracy: 0.3505\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2224 - accuracy: 0.3378 - val_loss: 2.1959 - val_accuracy: 0.3505\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2200 - accuracy: 0.3378 - val_loss: 2.1940 - val_accuracy: 0.3505\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2178 - accuracy: 0.3378 - val_loss: 2.1923 - val_accuracy: 0.3505\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2157 - accuracy: 0.3378 - val_loss: 2.1906 - val_accuracy: 0.3505\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.2137 - accuracy: 0.3378 - val_loss: 2.1891 - val_accuracy: 0.3505\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.2119 - accuracy: 0.3378 - val_loss: 2.1877 - val_accuracy: 0.3505\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2102 - accuracy: 0.3378 - val_loss: 2.1864 - val_accuracy: 0.3505\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.2085 - accuracy: 0.3378 - val_loss: 2.1852 - val_accuracy: 0.3505\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2070 - accuracy: 0.3378 - val_loss: 2.1840 - val_accuracy: 0.3505\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2056 - accuracy: 0.3378 - val_loss: 2.1829 - val_accuracy: 0.3505\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.2042 - accuracy: 0.3378 - val_loss: 2.1819 - val_accuracy: 0.3505\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2029 - accuracy: 0.3378 - val_loss: 2.1809 - val_accuracy: 0.3505\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2017 - accuracy: 0.3378 - val_loss: 2.1800 - val_accuracy: 0.3505\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2005 - accuracy: 0.3378 - val_loss: 2.1791 - val_accuracy: 0.3505\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1994 - accuracy: 0.3378 - val_loss: 2.1783 - val_accuracy: 0.3505\n",
      "Epoch 46/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1983 - accuracy: 0.3378 - val_loss: 2.1775 - val_accuracy: 0.3505\n",
      "Epoch 47/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1973 - accuracy: 0.3378 - val_loss: 2.1768 - val_accuracy: 0.3505\n",
      "Epoch 48/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1963 - accuracy: 0.3378 - val_loss: 2.1761 - val_accuracy: 0.3505\n",
      "Epoch 49/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1954 - accuracy: 0.3378 - val_loss: 2.1754 - val_accuracy: 0.3505\n",
      "Epoch 50/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1945 - accuracy: 0.3378 - val_loss: 2.1747 - val_accuracy: 0.3505\n",
      "Epoch 51/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1937 - accuracy: 0.3378 - val_loss: 2.1741 - val_accuracy: 0.3505\n",
      "Epoch 52/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1929 - accuracy: 0.3378 - val_loss: 2.1735 - val_accuracy: 0.3505\n",
      "Epoch 53/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1921 - accuracy: 0.3378 - val_loss: 2.1730 - val_accuracy: 0.3505\n",
      "Epoch 54/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1914 - accuracy: 0.3378 - val_loss: 2.1724 - val_accuracy: 0.3505\n",
      "Epoch 55/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1907 - accuracy: 0.3378 - val_loss: 2.1719 - val_accuracy: 0.3505\n",
      "Epoch 56/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1900 - accuracy: 0.3378 - val_loss: 2.1714 - val_accuracy: 0.3505\n",
      "Epoch 57/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1893 - accuracy: 0.3378 - val_loss: 2.1709 - val_accuracy: 0.3505\n",
      "Epoch 58/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1887 - accuracy: 0.3378 - val_loss: 2.1704 - val_accuracy: 0.3505\n",
      "Epoch 59/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1881 - accuracy: 0.3378 - val_loss: 2.1699 - val_accuracy: 0.3505\n",
      "Epoch 60/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1875 - accuracy: 0.3378 - val_loss: 2.1695 - val_accuracy: 0.3505\n",
      "Epoch 61/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1869 - accuracy: 0.3378 - val_loss: 2.1691 - val_accuracy: 0.3505\n",
      "Epoch 62/100\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.1864 - accuracy: 0.3378 - val_loss: 2.1687 - val_accuracy: 0.3505\n",
      "Epoch 63/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1859 - accuracy: 0.3378 - val_loss: 2.1683 - val_accuracy: 0.3505\n",
      "Epoch 64/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1854 - accuracy: 0.3378 - val_loss: 2.1679 - val_accuracy: 0.3505\n",
      "Epoch 65/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1848 - accuracy: 0.3378 - val_loss: 2.1675 - val_accuracy: 0.3505\n",
      "Epoch 66/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1844 - accuracy: 0.3378 - val_loss: 2.1672 - val_accuracy: 0.3505\n",
      "Epoch 67/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1839 - accuracy: 0.3378 - val_loss: 2.1668 - val_accuracy: 0.3505\n",
      "Epoch 68/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1835 - accuracy: 0.3378 - val_loss: 2.1664 - val_accuracy: 0.3505\n",
      "Epoch 69/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1831 - accuracy: 0.3378 - val_loss: 2.1661 - val_accuracy: 0.3505\n",
      "Epoch 70/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1826 - accuracy: 0.3378 - val_loss: 2.1658 - val_accuracy: 0.3505\n",
      "Epoch 71/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1822 - accuracy: 0.3378 - val_loss: 2.1654 - val_accuracy: 0.3505\n",
      "Epoch 72/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1818 - accuracy: 0.3378 - val_loss: 2.1651 - val_accuracy: 0.3505\n",
      "Epoch 73/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1815 - accuracy: 0.3378 - val_loss: 2.1648 - val_accuracy: 0.3505\n",
      "Epoch 74/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1811 - accuracy: 0.3378 - val_loss: 2.1645 - val_accuracy: 0.3505\n",
      "Epoch 75/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1807 - accuracy: 0.3378 - val_loss: 2.1642 - val_accuracy: 0.3505\n",
      "Epoch 76/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1804 - accuracy: 0.3378 - val_loss: 2.1639 - val_accuracy: 0.3505\n",
      "Epoch 77/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1801 - accuracy: 0.3378 - val_loss: 2.1637 - val_accuracy: 0.3505\n",
      "Epoch 78/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1797 - accuracy: 0.3378 - val_loss: 2.1634 - val_accuracy: 0.3505\n",
      "Epoch 79/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1794 - accuracy: 0.3378 - val_loss: 2.1631 - val_accuracy: 0.3505\n",
      "Epoch 80/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1791 - accuracy: 0.3378 - val_loss: 2.1629 - val_accuracy: 0.3505\n",
      "Epoch 81/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1788 - accuracy: 0.3378 - val_loss: 2.1626 - val_accuracy: 0.3505\n",
      "Epoch 82/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1785 - accuracy: 0.3378 - val_loss: 2.1624 - val_accuracy: 0.3505\n",
      "Epoch 83/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1783 - accuracy: 0.3378 - val_loss: 2.1621 - val_accuracy: 0.3505\n",
      "Epoch 84/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1780 - accuracy: 0.3378 - val_loss: 2.1619 - val_accuracy: 0.3505\n",
      "Epoch 85/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1777 - accuracy: 0.3378 - val_loss: 2.1617 - val_accuracy: 0.3505\n",
      "Epoch 86/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1775 - accuracy: 0.3378 - val_loss: 2.1614 - val_accuracy: 0.3505\n",
      "Epoch 87/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1772 - accuracy: 0.3378 - val_loss: 2.1612 - val_accuracy: 0.3505\n",
      "Epoch 88/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1769 - accuracy: 0.3378 - val_loss: 2.1610 - val_accuracy: 0.3505\n",
      "Epoch 89/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1767 - accuracy: 0.3378 - val_loss: 2.1608 - val_accuracy: 0.3505\n",
      "Epoch 90/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1765 - accuracy: 0.3378 - val_loss: 2.1606 - val_accuracy: 0.3505\n",
      "Epoch 91/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1762 - accuracy: 0.3378 - val_loss: 2.1604 - val_accuracy: 0.3505\n",
      "Epoch 92/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1760 - accuracy: 0.3378 - val_loss: 2.1602 - val_accuracy: 0.3505\n",
      "Epoch 93/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1758 - accuracy: 0.3378 - val_loss: 2.1600 - val_accuracy: 0.3505\n",
      "Epoch 94/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1756 - accuracy: 0.3378 - val_loss: 2.1598 - val_accuracy: 0.3505\n",
      "Epoch 95/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1754 - accuracy: 0.3378 - val_loss: 2.1596 - val_accuracy: 0.3505\n",
      "Epoch 96/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1752 - accuracy: 0.3378 - val_loss: 2.1594 - val_accuracy: 0.3505\n",
      "Epoch 97/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1750 - accuracy: 0.3378 - val_loss: 2.1593 - val_accuracy: 0.3505\n",
      "Epoch 98/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1748 - accuracy: 0.3378 - val_loss: 2.1591 - val_accuracy: 0.3505\n",
      "Epoch 99/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1746 - accuracy: 0.3378 - val_loss: 2.1589 - val_accuracy: 0.3505\n",
      "Epoch 100/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1744 - accuracy: 0.3378 - val_loss: 2.1588 - val_accuracy: 0.3505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb575489460>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "            epochs=100,\n",
    "            batch_size=64, \n",
    "            validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 2ms/step - loss: 2.2466 - accuracy: 0.2859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2466201782226562, 0.2858695685863495]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative modele\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.01, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "input_tensor = tf.keras.Input(shape=[16, 1536])\n",
    "layer = MultiHeadAttention(num_heads=5, key_dim=5, attention_axes=(1, 2))\n",
    "x = layer(input_tensor, input_tensor)\n",
    "layer_norm = tf.keras.layers.LayerNormalization()\n",
    "x = layer_norm(x)\n",
    "\n",
    "d_model = 1536\n",
    "\n",
    "seq = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(2048, activation='relu'),\n",
    "    tf.keras.layers.Dense(d_model, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1)])\n",
    "\n",
    "add = tf.keras.layers.Add()\n",
    "x = add([x, seq(x)])\n",
    "layer_norm = tf.keras.layers.LayerNormalization()\n",
    "x = layer_norm(x)\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "final = tf.keras.layers.Dense(1536, activation='tanh')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_tensor, outputs=final)\n",
    "model.compile(loss='cosine_similarity', optimizer='adam', metrics=['cosine_similarity'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "            epochs=1000,\n",
    "            batch_size=64, \n",
    "            validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_54 (InputLayer)       [(None, 16, 128)]            0         []                            \n",
      "                                                                                                  \n",
      " multi_head_attention_42 (M  (None, 16, 128)              4763      ['input_54[0][0]',            \n",
      " ultiHeadAttention)                                                  'input_54[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_38 (La  (None, 16, 128)              256       ['multi_head_attention_42[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " sequential_27 (Sequential)  (None, 16, 128)              526464    ['layer_normalization_38[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_27 (Add)                (None, 16, 128)              0         ['layer_normalization_38[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'sequential_27[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_39 (La  (None, 16, 128)              256       ['add_27[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " flatten_14 (Flatten)        (None, 2048)                 0         ['layer_normalization_39[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_94 (Dense)            (None, 128)                  262272    ['flatten_14[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 794011 (3.03 MB)\n",
      "Trainable params: 794011 (3.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/111 [..............................] - ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 0s 2ms/step\n",
      "0.32\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tp = 0\n",
    "fp = 0\n",
    "\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "for i in range(100):\n",
    "    p = np.argmax(cosine_similarity(X_test[i], [pred[i]])) == np.argmax(cosine_similarity(X_test[i], [y_test[i]]))\n",
    "    if p:\n",
    "        tp += 1\n",
    "    fp += 1\n",
    "\n",
    "print(tp/fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "\n",
    "\n",
    "df = pd.read_pickle('../../data/datasets/raw-video-level-watches')\n",
    "videos = pd.read_pickle('../../data/videos_raw_metadata')\n",
    "title_embeddings = pd.read_pickle('../../data/embeddings/openai-title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "playing = watch['playing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01953424, -0.01826318,  0.0135201 , ...,  0.00290338,\n",
       "       -0.00709789,  0.00755279])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(title_embeddings[videos[playing]['snippet']['title']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ca7ed9ad41451da70624b183043025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Categorical\n",
    "\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for watch in tqdm(df):\n",
    "    playing = watch['playing']\n",
    "    selected = watch['selected']\n",
    "    if playing not in videos or selected not in videos or any([s not in videos for s in watch['upnext']]):\n",
    "        continue\n",
    "\n",
    "    if len(watch['upnext']) < 15:\n",
    "        continue\n",
    "        \n",
    "    upnext = watch['upnext'][:15]\n",
    "\n",
    "    playing = watch['playing']\n",
    "    playing = np.array(title_embeddings[videos[playing]['snippet']['title']])\n",
    "    all_suggested = [np.array(title_embeddings[videos[s]['snippet']['title']]) for s in upnext]\n",
    "\n",
    "    selected = watch['selected']\n",
    "    y = [0]\n",
    "    for s in upnext:\n",
    "        if s == selected:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "\n",
    "    x = np.vstack([playing, all_suggested])\n",
    "    X.append(x)\n",
    "    Y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4599, 16)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4599, 16, 1536)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71721c5f5fba4628ba23d1a5f7d40f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generative\n",
    "\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for watch in tqdm(df):\n",
    "    playing = watch['playing']\n",
    "    selected = watch['selected']\n",
    "    if playing not in videos or selected not in videos or any([s not in videos for s in watch['upnext']]):\n",
    "        continue\n",
    "\n",
    "    if len(watch['upnext']) < 15:\n",
    "        continue\n",
    "        \n",
    "    upnext = watch['upnext'][:15]\n",
    "\n",
    "    playing = watch['playing']\n",
    "    playing = np.array(title_embeddings[videos[playing]['snippet']['title']])\n",
    "    all_suggested = [np.array(title_embeddings[videos[s]['snippet']['title']]) for s in upnext]\n",
    "\n",
    "    selected = watch['selected']\n",
    "    y = []\n",
    "    for s in upnext:\n",
    "        if s == selected:\n",
    "            y = np.array(title_embeddings[videos[selected]['snippet']['title']])\n",
    "\n",
    "    if len(y) == 0:\n",
    "        continue\n",
    "\n",
    "    x = np.vstack([playing, all_suggested])\n",
    "    X.append(x)\n",
    "    Y.append(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4413, 16, 1536)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26caa140dcc24604b0deb6fb09df914c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generative seperate\n",
    "\n",
    "pX = []\n",
    "sX = []\n",
    "Y = []\n",
    "for watch in tqdm(df):\n",
    "    playing = watch['playing']\n",
    "    selected = watch['selected']\n",
    "    if playing not in videos or selected not in videos or any([s not in videos for s in watch['upnext']]):\n",
    "        continue\n",
    "\n",
    "    if len(watch['upnext']) < 15:\n",
    "        continue\n",
    "        \n",
    "    upnext = watch['upnext'][:15]\n",
    "\n",
    "    playing = watch['playing']\n",
    "    playing = np.array(title_embeddings[videos[playing]['snippet']['title']])\n",
    "    all_suggested = [np.array(title_embeddings[videos[s]['snippet']['title']]) for s in upnext]\n",
    "\n",
    "    selected = watch['selected']\n",
    "    y = []\n",
    "    for s in upnext:\n",
    "        if s == selected:\n",
    "            y = np.array(title_embeddings[videos[selected]['snippet']['title']])\n",
    "\n",
    "    if len(y) == 0:\n",
    "        continue\n",
    "\n",
    "    pX.append(playing)\n",
    "    sX.append(all_suggested)\n",
    "    Y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4413, 15, 1536)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sX = np.array(sX)\n",
    "pX = np.array(pX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4413, 1, 1536)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dbd0840a53844d6bd3355959f3bc330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generative seperate\n",
    "\n",
    "pX = []\n",
    "sX = []\n",
    "isX = []\n",
    "Y = []\n",
    "for watch in tqdm(df):\n",
    "    playing = watch['playing']\n",
    "    selected = watch['selected']\n",
    "    if playing not in videos or selected not in videos or any([s not in videos for s in watch['upnext']]):\n",
    "        continue\n",
    "\n",
    "    if len(watch['upnext']) < 15:\n",
    "        continue\n",
    "        \n",
    "    upnext = watch['upnext'][:15]\n",
    "\n",
    "    playing = watch['playing']\n",
    "    playing = np.array(title_embeddings[videos[playing]['snippet']['title']])\n",
    "    all_suggested = [np.array(title_embeddings[videos[s]['snippet']['title']]) for s in upnext]\n",
    "\n",
    "    selected = watch['selected']\n",
    "    y = []\n",
    "    for s in upnext:\n",
    "        if s == selected:\n",
    "            y = 1\n",
    "        else:\n",
    "            y = 0\n",
    "\n",
    "        s = np.array(title_embeddings[videos[s]['snippet']['title']])\n",
    "\n",
    "        pX.append(playing)\n",
    "        sX.append(all_suggested)\n",
    "        isX.append(s)\n",
    "        Y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(Y)\n",
    "pX = np.array(pX)\n",
    "pX = pX.reshape(pX.shape[0], 1, pX.shape[1])\n",
    "isX = np.array(isX)\n",
    "isX = isX.reshape(isX.shape[0], 1, isX.shape[1])\n",
    "sX = np.array(sX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pX_train, pX_test, isX_train, isX_test, sX_train, sX_test, y_train, y_test = train_test_split(pX, isX, sX, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55188, 1, 1536),\n",
       " (13797, 1, 1536),\n",
       " (55188, 1, 1536),\n",
       " (13797, 1, 1536),\n",
       " (55188, 15, 1536),\n",
       " (13797, 15, 1536),\n",
       " (55188,),\n",
       " (13797,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pX_train.shape, pX_test.shape, isX_train.shape, isX_test.shape, sX_train.shape, sX_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 1920])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102.4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_31 (InputLayer)       [(None, 15, 1536)]           0         []                            \n",
      "                                                                                                  \n",
      " dense_47 (Dense)            (None, 15, 512)              786944    ['input_31[0][0]']            \n",
      "                                                                                                  \n",
      " input_32 (InputLayer)       [(None, 1, 1536)]            0         []                            \n",
      "                                                                                                  \n",
      " dense_48 (Dense)            (None, 15, 64)               32832     ['dense_47[0][0]']            \n",
      "                                                                                                  \n",
      " dense_49 (Dense)            (None, 1, 960)               1475520   ['input_32[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_19 (Flatten)        (None, 960)                  0         ['dense_48[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_20 (Flatten)        (None, 960)                  0         ['dense_49[0][0]']            \n",
      "                                                                                                  \n",
      " input_33 (InputLayer)       [(None, 1, 1536)]            0         []                            \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate  (None, 1920)                 0         ['flatten_19[0][0]',          \n",
      " )                                                                   'flatten_20[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_21 (Flatten)        (None, 1536)                 0         ['input_33[0][0]']            \n",
      "                                                                                                  \n",
      " dense_50 (Dense)            (None, 512)                  983552    ['concatenate_9[0][0]']       \n",
      "                                                                                                  \n",
      " dense_51 (Dense)            (None, 512)                  786944    ['flatten_21[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenat  (None, 1024)                 0         ['dense_50[0][0]',            \n",
      " e)                                                                  'dense_51[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_22 (Flatten)        (None, 1024)                 0         ['concatenate_10[0][0]']      \n",
      "                                                                                                  \n",
      " dense_52 (Dense)            (None, 512)                  524800    ['flatten_22[0][0]']          \n",
      "                                                                                                  \n",
      " dense_53 (Dense)            (None, 1)                    513       ['dense_52[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4591105 (17.51 MB)\n",
      "Trainable params: 4591105 (17.51 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# binary separate inputs\n",
    "\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "suggested_tensor = tf.keras.Input(shape=[15, 1536])\n",
    "playing_tensor = tf.keras.Input(shape=[1, 1536])\n",
    "# layer = MultiHeadAttention(num_heads=15, key_dim=15, attention_axes=(1), name='attention')\n",
    "# # x = layer(playing_tensor, suggested_tensor)\n",
    "# x = layer(suggested_tensor, playing_tensor)\n",
    "# x = layer(suggested_tensor, suggested_tensor)\n",
    "# layer_norm = tf.keras.layers.LayerNormalization()\n",
    "# x = layer_norm(x)\n",
    "\n",
    "st_dense = tf.keras.layers.Dense(512, activation='relu')(suggested_tensor)\n",
    "st_dense = tf.keras.layers.Dense(64, activation='relu')(st_dense)\n",
    "st = tf.keras.layers.Flatten()(st_dense)\n",
    "\n",
    "pt = tf.keras.layers.Dense(960, activation='relu')(playing_tensor)\n",
    "pt = tf.keras.layers.Flatten()(pt)\n",
    "\n",
    "merged = tf.keras.layers.Concatenate()([st, pt])\n",
    "mem = tf.keras.layers.Dense(512, activation='relu')(merged)\n",
    "\n",
    "is_tensor = tf.keras.Input(shape=[1, 1536])\n",
    "is_t = tf.keras.layers.Flatten()(is_tensor)\n",
    "is_t = tf.keras.layers.Dense(512, activation='relu')(is_t)\n",
    "\n",
    "merged = tf.keras.layers.Concatenate()([mem, is_t])\n",
    "\n",
    "x = tf.keras.layers.Flatten()(merged)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "final = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[suggested_tensor, playing_tensor, is_tensor], outputs=final)\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='acc'),\n",
    "      keras.metrics.Precision(name='P'),\n",
    "      keras.metrics.Recall(name='R'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=METRICS)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.534742570113018, 1: 7.69578313253012}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = Y.sum()\n",
    "neg = len(Y) - pos\n",
    "total = neg + pos\n",
    "w0 = (1 / neg) * (total / 2.0)\n",
    "w1 = (1 / pos) * (total / 2.0)\n",
    "class_weights = {0:w0, 1:w1}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "777/777 [==============================] - 10s 10ms/step - loss: 0.6955 - tp: 1796.0000 - fp: 25627.0000 - tn: 20798.0000 - fn: 1448.0000 - acc: 0.4549 - P: 0.0655 - R: 0.5536 - auc: 0.5015 - prc: 0.0657 - val_loss: 0.6928 - val_tp: 207.0000 - val_fp: 2842.0000 - val_tn: 2333.0000 - val_fn: 137.0000 - val_acc: 0.4602 - val_P: 0.0679 - val_R: 0.6017 - val_auc: 0.5296 - val_prc: 0.0702\n",
      "Epoch 2/1000\n",
      "777/777 [==============================] - 7s 8ms/step - loss: 0.6933 - tp: 1732.0000 - fp: 22246.0000 - tn: 24179.0000 - fn: 1512.0000 - acc: 0.5217 - P: 0.0722 - R: 0.5339 - auc: 0.5369 - prc: 0.0728 - val_loss: 0.7196 - val_tp: 220.0000 - val_fp: 3187.0000 - val_tn: 1988.0000 - val_fn: 124.0000 - val_acc: 0.4001 - val_P: 0.0646 - val_R: 0.6395 - val_auc: 0.5298 - val_prc: 0.0713\n",
      "Epoch 3/1000\n",
      "777/777 [==============================] - 6s 8ms/step - loss: 0.6858 - tp: 1975.0000 - fp: 23408.0000 - tn: 23017.0000 - fn: 1269.0000 - acc: 0.5032 - P: 0.0778 - R: 0.6088 - auc: 0.5742 - prc: 0.0817 - val_loss: 0.7888 - val_tp: 296.0000 - val_fp: 4238.0000 - val_tn: 937.0000 - val_fn: 48.0000 - val_acc: 0.2234 - val_P: 0.0653 - val_R: 0.8605 - val_auc: 0.5400 - val_prc: 0.0734\n",
      "Epoch 4/1000\n",
      "777/777 [==============================] - 6s 8ms/step - loss: 0.6783 - tp: 2010.0000 - fp: 21775.0000 - tn: 24650.0000 - fn: 1234.0000 - acc: 0.5368 - P: 0.0845 - R: 0.6196 - auc: 0.6013 - prc: 0.0937 - val_loss: 0.8816 - val_tp: 318.0000 - val_fp: 4714.0000 - val_tn: 461.0000 - val_fn: 26.0000 - val_acc: 0.1411 - val_P: 0.0632 - val_R: 0.9244 - val_auc: 0.5402 - val_prc: 0.0746\n",
      "Epoch 5/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.6697 - tp: 2078.0000 - fp: 21494.0000 - tn: 24931.0000 - fn: 1166.0000 - acc: 0.5438 - P: 0.0882 - R: 0.6406 - auc: 0.6237 - prc: 0.1038 - val_loss: 0.7066 - val_tp: 213.0000 - val_fp: 2899.0000 - val_tn: 2276.0000 - val_fn: 131.0000 - val_acc: 0.4510 - val_P: 0.0684 - val_R: 0.6192 - val_auc: 0.5496 - val_prc: 0.0764\n",
      "Epoch 6/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.6553 - tp: 2154.0000 - fp: 20915.0000 - tn: 25510.0000 - fn: 1090.0000 - acc: 0.5570 - P: 0.0934 - R: 0.6640 - auc: 0.6506 - prc: 0.1180 - val_loss: 0.5679 - val_tp: 100.0000 - val_fp: 1283.0000 - val_tn: 3892.0000 - val_fn: 244.0000 - val_acc: 0.7233 - val_P: 0.0723 - val_R: 0.2907 - val_auc: 0.5375 - val_prc: 0.0726\n",
      "Epoch 7/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.6333 - tp: 2230.0000 - fp: 19246.0000 - tn: 27179.0000 - fn: 1014.0000 - acc: 0.5921 - P: 0.1038 - R: 0.6874 - auc: 0.6899 - prc: 0.1399 - val_loss: 0.7877 - val_tp: 208.0000 - val_fp: 2944.0000 - val_tn: 2231.0000 - val_fn: 136.0000 - val_acc: 0.4419 - val_P: 0.0660 - val_R: 0.6047 - val_auc: 0.5438 - val_prc: 0.0768\n",
      "Epoch 8/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.6003 - tp: 2335.0000 - fp: 17692.0000 - tn: 28733.0000 - fn: 909.0000 - acc: 0.6255 - P: 0.1166 - R: 0.7198 - auc: 0.7357 - prc: 0.1744 - val_loss: 0.6538 - val_tp: 131.0000 - val_fp: 2082.0000 - val_tn: 3093.0000 - val_fn: 213.0000 - val_acc: 0.5842 - val_P: 0.0592 - val_R: 0.3808 - val_auc: 0.5263 - val_prc: 0.0707\n",
      "Epoch 9/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.5508 - tp: 2456.0000 - fp: 15615.0000 - tn: 30810.0000 - fn: 788.0000 - acc: 0.6698 - P: 0.1359 - R: 0.7571 - auc: 0.7867 - prc: 0.2270 - val_loss: 0.7138 - val_tp: 169.0000 - val_fp: 2296.0000 - val_tn: 2879.0000 - val_fn: 175.0000 - val_acc: 0.5523 - val_P: 0.0686 - val_R: 0.4913 - val_auc: 0.5335 - val_prc: 0.0718\n",
      "Epoch 10/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.4963 - tp: 2584.0000 - fp: 13879.0000 - tn: 32546.0000 - fn: 660.0000 - acc: 0.7073 - P: 0.1570 - R: 0.7965 - auc: 0.8341 - prc: 0.2862 - val_loss: 0.5112 - val_tp: 92.0000 - val_fp: 1118.0000 - val_tn: 4057.0000 - val_fn: 252.0000 - val_acc: 0.7518 - val_P: 0.0760 - val_R: 0.2674 - val_auc: 0.5350 - val_prc: 0.0713\n",
      "Epoch 11/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.4521 - tp: 2668.0000 - fp: 12170.0000 - tn: 34255.0000 - fn: 576.0000 - acc: 0.7434 - P: 0.1798 - R: 0.8224 - auc: 0.8649 - prc: 0.3335 - val_loss: 0.6065 - val_tp: 112.0000 - val_fp: 1502.0000 - val_tn: 3673.0000 - val_fn: 232.0000 - val_acc: 0.6858 - val_P: 0.0694 - val_R: 0.3256 - val_auc: 0.5310 - val_prc: 0.0702\n",
      "Epoch 12/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.4063 - tp: 2763.0000 - fp: 10957.0000 - tn: 35468.0000 - fn: 481.0000 - acc: 0.7697 - P: 0.2014 - R: 0.8517 - auc: 0.8923 - prc: 0.3871 - val_loss: 0.5468 - val_tp: 104.0000 - val_fp: 1188.0000 - val_tn: 3987.0000 - val_fn: 240.0000 - val_acc: 0.7413 - val_P: 0.0805 - val_R: 0.3023 - val_auc: 0.5369 - val_prc: 0.0726\n",
      "Epoch 13/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.3716 - tp: 2824.0000 - fp: 9887.0000 - tn: 36538.0000 - fn: 420.0000 - acc: 0.7925 - P: 0.2222 - R: 0.8705 - auc: 0.9102 - prc: 0.4316 - val_loss: 0.5372 - val_tp: 92.0000 - val_fp: 1071.0000 - val_tn: 4104.0000 - val_fn: 252.0000 - val_acc: 0.7603 - val_P: 0.0791 - val_R: 0.2674 - val_auc: 0.5430 - val_prc: 0.0719\n",
      "Epoch 14/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.3415 - tp: 2900.0000 - fp: 9026.0000 - tn: 37399.0000 - fn: 344.0000 - acc: 0.8114 - P: 0.2432 - R: 0.8940 - auc: 0.9239 - prc: 0.4610 - val_loss: 0.6680 - val_tp: 127.0000 - val_fp: 1473.0000 - val_tn: 3702.0000 - val_fn: 217.0000 - val_acc: 0.6938 - val_P: 0.0794 - val_R: 0.3692 - val_auc: 0.5502 - val_prc: 0.0760\n",
      "Epoch 15/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.3156 - tp: 2947.0000 - fp: 8369.0000 - tn: 38056.0000 - fn: 297.0000 - acc: 0.8255 - P: 0.2604 - R: 0.9084 - auc: 0.9344 - prc: 0.4970 - val_loss: 0.7117 - val_tp: 121.0000 - val_fp: 1535.0000 - val_tn: 3640.0000 - val_fn: 223.0000 - val_acc: 0.6815 - val_P: 0.0731 - val_R: 0.3517 - val_auc: 0.5350 - val_prc: 0.0726\n",
      "Epoch 16/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.2948 - tp: 2970.0000 - fp: 7785.0000 - tn: 38640.0000 - fn: 274.0000 - acc: 0.8377 - P: 0.2762 - R: 0.9155 - auc: 0.9423 - prc: 0.5191 - val_loss: 0.5805 - val_tp: 83.0000 - val_fp: 927.0000 - val_tn: 4248.0000 - val_fn: 261.0000 - val_acc: 0.7847 - val_P: 0.0822 - val_R: 0.2413 - val_auc: 0.5318 - val_prc: 0.0721\n",
      "Epoch 17/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.2725 - tp: 3002.0000 - fp: 6978.0000 - tn: 39447.0000 - fn: 242.0000 - acc: 0.8546 - P: 0.3008 - R: 0.9254 - auc: 0.9501 - prc: 0.5451 - val_loss: 0.6186 - val_tp: 94.0000 - val_fp: 937.0000 - val_tn: 4238.0000 - val_fn: 250.0000 - val_acc: 0.7849 - val_P: 0.0912 - val_R: 0.2733 - val_auc: 0.5392 - val_prc: 0.0744\n",
      "Epoch 18/1000\n",
      "777/777 [==============================] - 6s 8ms/step - loss: 0.2647 - tp: 3033.0000 - fp: 6885.0000 - tn: 39540.0000 - fn: 211.0000 - acc: 0.8571 - P: 0.3058 - R: 0.9350 - auc: 0.9532 - prc: 0.5586 - val_loss: 0.6313 - val_tp: 82.0000 - val_fp: 924.0000 - val_tn: 4251.0000 - val_fn: 262.0000 - val_acc: 0.7851 - val_P: 0.0815 - val_R: 0.2384 - val_auc: 0.5384 - val_prc: 0.0746\n",
      "Epoch 19/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.2449 - tp: 3032.0000 - fp: 6301.0000 - tn: 40124.0000 - fn: 212.0000 - acc: 0.8689 - P: 0.3249 - R: 0.9346 - auc: 0.9589 - prc: 0.5946 - val_loss: 0.6187 - val_tp: 70.0000 - val_fp: 832.0000 - val_tn: 4343.0000 - val_fn: 274.0000 - val_acc: 0.7996 - val_P: 0.0776 - val_R: 0.2035 - val_auc: 0.5332 - val_prc: 0.0717\n",
      "Epoch 20/1000\n",
      "777/777 [==============================] - 6s 8ms/step - loss: 0.2326 - tp: 3060.0000 - fp: 5928.0000 - tn: 40497.0000 - fn: 184.0000 - acc: 0.8769 - P: 0.3405 - R: 0.9433 - auc: 0.9624 - prc: 0.6099 - val_loss: 0.6362 - val_tp: 72.0000 - val_fp: 812.0000 - val_tn: 4363.0000 - val_fn: 272.0000 - val_acc: 0.8036 - val_P: 0.0814 - val_R: 0.2093 - val_auc: 0.5252 - val_prc: 0.0700\n",
      "Epoch 21/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.2299 - tp: 3051.0000 - fp: 5798.0000 - tn: 40627.0000 - fn: 193.0000 - acc: 0.8794 - P: 0.3448 - R: 0.9405 - auc: 0.9636 - prc: 0.6229 - val_loss: 0.6052 - val_tp: 61.0000 - val_fp: 721.0000 - val_tn: 4454.0000 - val_fn: 283.0000 - val_acc: 0.8181 - val_P: 0.0780 - val_R: 0.1773 - val_auc: 0.5214 - val_prc: 0.0685\n",
      "Epoch 22/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.2132 - tp: 3085.0000 - fp: 5375.0000 - tn: 41050.0000 - fn: 159.0000 - acc: 0.8886 - P: 0.3647 - R: 0.9510 - auc: 0.9676 - prc: 0.6322 - val_loss: 0.6391 - val_tp: 67.0000 - val_fp: 749.0000 - val_tn: 4426.0000 - val_fn: 277.0000 - val_acc: 0.8141 - val_P: 0.0821 - val_R: 0.1948 - val_auc: 0.5218 - val_prc: 0.0693\n",
      "Epoch 23/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.2072 - tp: 3099.0000 - fp: 5227.0000 - tn: 41198.0000 - fn: 145.0000 - acc: 0.8918 - P: 0.3722 - R: 0.9553 - auc: 0.9686 - prc: 0.6386 - val_loss: 0.7572 - val_tp: 79.0000 - val_fp: 961.0000 - val_tn: 4214.0000 - val_fn: 265.0000 - val_acc: 0.7779 - val_P: 0.0760 - val_R: 0.2297 - val_auc: 0.5276 - val_prc: 0.0724\n",
      "Epoch 24/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.2034 - tp: 3100.0000 - fp: 5039.0000 - tn: 41386.0000 - fn: 144.0000 - acc: 0.8956 - P: 0.3809 - R: 0.9556 - auc: 0.9699 - prc: 0.6508 - val_loss: 0.6626 - val_tp: 69.0000 - val_fp: 737.0000 - val_tn: 4438.0000 - val_fn: 275.0000 - val_acc: 0.8166 - val_P: 0.0856 - val_R: 0.2006 - val_auc: 0.5294 - val_prc: 0.0713\n",
      "Epoch 25/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.1905 - tp: 3122.0000 - fp: 4668.0000 - tn: 41757.0000 - fn: 122.0000 - acc: 0.9036 - P: 0.4008 - R: 0.9624 - auc: 0.9730 - prc: 0.6732 - val_loss: 0.7051 - val_tp: 64.0000 - val_fp: 692.0000 - val_tn: 4483.0000 - val_fn: 280.0000 - val_acc: 0.8239 - val_P: 0.0847 - val_R: 0.1860 - val_auc: 0.5285 - val_prc: 0.0726\n",
      "Epoch 26/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.1879 - tp: 3103.0000 - fp: 4567.0000 - tn: 41858.0000 - fn: 141.0000 - acc: 0.9052 - P: 0.4046 - R: 0.9565 - auc: 0.9736 - prc: 0.6755 - val_loss: 0.7330 - val_tp: 68.0000 - val_fp: 755.0000 - val_tn: 4420.0000 - val_fn: 276.0000 - val_acc: 0.8132 - val_P: 0.0826 - val_R: 0.1977 - val_auc: 0.5117 - val_prc: 0.0685\n",
      "Epoch 27/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.1723 - tp: 3139.0000 - fp: 4275.0000 - tn: 42150.0000 - fn: 105.0000 - acc: 0.9118 - P: 0.4234 - R: 0.9676 - auc: 0.9767 - prc: 0.6969 - val_loss: 0.7635 - val_tp: 70.0000 - val_fp: 747.0000 - val_tn: 4428.0000 - val_fn: 274.0000 - val_acc: 0.8150 - val_P: 0.0857 - val_R: 0.2035 - val_auc: 0.5323 - val_prc: 0.0739\n",
      "Epoch 28/1000\n",
      "777/777 [==============================] - 7s 8ms/step - loss: 0.1701 - tp: 3142.0000 - fp: 4110.0000 - tn: 42315.0000 - fn: 102.0000 - acc: 0.9152 - P: 0.4333 - R: 0.9686 - auc: 0.9775 - prc: 0.7084 - val_loss: 0.8134 - val_tp: 58.0000 - val_fp: 717.0000 - val_tn: 4458.0000 - val_fn: 286.0000 - val_acc: 0.8183 - val_P: 0.0748 - val_R: 0.1686 - val_auc: 0.5108 - val_prc: 0.0665\n",
      "Epoch 29/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.1663 - tp: 3148.0000 - fp: 4032.0000 - tn: 42393.0000 - fn: 96.0000 - acc: 0.9169 - P: 0.4384 - R: 0.9704 - auc: 0.9781 - prc: 0.7130 - val_loss: 0.7593 - val_tp: 53.0000 - val_fp: 553.0000 - val_tn: 4622.0000 - val_fn: 291.0000 - val_acc: 0.8471 - val_P: 0.0875 - val_R: 0.1541 - val_auc: 0.5144 - val_prc: 0.0682\n",
      "Epoch 30/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.1618 - tp: 3153.0000 - fp: 3919.0000 - tn: 42506.0000 - fn: 91.0000 - acc: 0.9193 - P: 0.4458 - R: 0.9719 - auc: 0.9791 - prc: 0.7259 - val_loss: 0.7879 - val_tp: 58.0000 - val_fp: 641.0000 - val_tn: 4534.0000 - val_fn: 286.0000 - val_acc: 0.8320 - val_P: 0.0830 - val_R: 0.1686 - val_auc: 0.5079 - val_prc: 0.0682\n",
      "Epoch 31/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.1596 - tp: 3150.0000 - fp: 3757.0000 - tn: 42668.0000 - fn: 94.0000 - acc: 0.9225 - P: 0.4561 - R: 0.9710 - auc: 0.9796 - prc: 0.7247 - val_loss: 0.7320 - val_tp: 68.0000 - val_fp: 648.0000 - val_tn: 4527.0000 - val_fn: 276.0000 - val_acc: 0.8326 - val_P: 0.0950 - val_R: 0.1977 - val_auc: 0.5318 - val_prc: 0.0766\n",
      "Epoch 32/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.1514 - tp: 3165.0000 - fp: 3598.0000 - tn: 42827.0000 - fn: 79.0000 - acc: 0.9260 - P: 0.4680 - R: 0.9756 - auc: 0.9811 - prc: 0.7356 - val_loss: 0.8238 - val_tp: 71.0000 - val_fp: 810.0000 - val_tn: 4365.0000 - val_fn: 273.0000 - val_acc: 0.8038 - val_P: 0.0806 - val_R: 0.2064 - val_auc: 0.5259 - val_prc: 0.0702\n",
      "Epoch 33/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.1535 - tp: 3152.0000 - fp: 3685.0000 - tn: 42740.0000 - fn: 92.0000 - acc: 0.9240 - P: 0.4610 - R: 0.9716 - auc: 0.9807 - prc: 0.7302 - val_loss: 0.7676 - val_tp: 68.0000 - val_fp: 766.0000 - val_tn: 4409.0000 - val_fn: 276.0000 - val_acc: 0.8112 - val_P: 0.0815 - val_R: 0.1977 - val_auc: 0.5225 - val_prc: 0.0710\n",
      "Epoch 34/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.1431 - tp: 3170.0000 - fp: 3394.0000 - tn: 43031.0000 - fn: 74.0000 - acc: 0.9302 - P: 0.4829 - R: 0.9772 - auc: 0.9826 - prc: 0.7476 - val_loss: 0.8200 - val_tp: 56.0000 - val_fp: 612.0000 - val_tn: 4563.0000 - val_fn: 288.0000 - val_acc: 0.8369 - val_P: 0.0838 - val_R: 0.1628 - val_auc: 0.5142 - val_prc: 0.0706\n",
      "Epoch 35/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.1335 - tp: 3176.0000 - fp: 3178.0000 - tn: 43247.0000 - fn: 68.0000 - acc: 0.9346 - P: 0.4998 - R: 0.9790 - auc: 0.9840 - prc: 0.7574 - val_loss: 0.8325 - val_tp: 53.0000 - val_fp: 608.0000 - val_tn: 4567.0000 - val_fn: 291.0000 - val_acc: 0.8371 - val_P: 0.0802 - val_R: 0.1541 - val_auc: 0.5213 - val_prc: 0.0695\n",
      "Epoch 36/1000\n",
      "777/777 [==============================] - 7s 8ms/step - loss: 0.1338 - tp: 3165.0000 - fp: 3077.0000 - tn: 43348.0000 - fn: 79.0000 - acc: 0.9365 - P: 0.5070 - R: 0.9756 - auc: 0.9842 - prc: 0.7665 - val_loss: 0.8297 - val_tp: 62.0000 - val_fp: 607.0000 - val_tn: 4568.0000 - val_fn: 282.0000 - val_acc: 0.8389 - val_P: 0.0927 - val_R: 0.1802 - val_auc: 0.5267 - val_prc: 0.0731\n",
      "Epoch 37/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.1283 - tp: 3176.0000 - fp: 2999.0000 - tn: 43426.0000 - fn: 68.0000 - acc: 0.9383 - P: 0.5143 - R: 0.9790 - auc: 0.9850 - prc: 0.7704 - val_loss: 0.8283 - val_tp: 61.0000 - val_fp: 595.0000 - val_tn: 4580.0000 - val_fn: 283.0000 - val_acc: 0.8409 - val_P: 0.0930 - val_R: 0.1773 - val_auc: 0.5251 - val_prc: 0.0720\n",
      "Epoch 38/1000\n",
      "777/777 [==============================] - 6s 8ms/step - loss: 0.1296 - tp: 3174.0000 - fp: 3064.0000 - tn: 43361.0000 - fn: 70.0000 - acc: 0.9369 - P: 0.5088 - R: 0.9784 - auc: 0.9850 - prc: 0.7726 - val_loss: 0.8649 - val_tp: 53.0000 - val_fp: 501.0000 - val_tn: 4674.0000 - val_fn: 291.0000 - val_acc: 0.8565 - val_P: 0.0957 - val_R: 0.1541 - val_auc: 0.5212 - val_prc: 0.0703\n",
      "Epoch 39/1000\n",
      "777/777 [==============================] - 6s 8ms/step - loss: 0.1246 - tp: 3179.0000 - fp: 2859.0000 - tn: 43566.0000 - fn: 65.0000 - acc: 0.9411 - P: 0.5265 - R: 0.9800 - auc: 0.9858 - prc: 0.7804 - val_loss: 0.8128 - val_tp: 53.0000 - val_fp: 539.0000 - val_tn: 4636.0000 - val_fn: 291.0000 - val_acc: 0.8496 - val_P: 0.0895 - val_R: 0.1541 - val_auc: 0.5203 - val_prc: 0.0701\n",
      "Epoch 40/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.1177 - tp: 3173.0000 - fp: 2731.0000 - tn: 43694.0000 - fn: 71.0000 - acc: 0.9436 - P: 0.5374 - R: 0.9781 - auc: 0.9871 - prc: 0.7892 - val_loss: 0.8881 - val_tp: 57.0000 - val_fp: 582.0000 - val_tn: 4593.0000 - val_fn: 287.0000 - val_acc: 0.8425 - val_P: 0.0892 - val_R: 0.1657 - val_auc: 0.5208 - val_prc: 0.0694\n",
      "Epoch 41/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.1202 - tp: 3182.0000 - fp: 2769.0000 - tn: 43656.0000 - fn: 62.0000 - acc: 0.9430 - P: 0.5347 - R: 0.9809 - auc: 0.9867 - prc: 0.7927 - val_loss: 0.8346 - val_tp: 60.0000 - val_fp: 612.0000 - val_tn: 4563.0000 - val_fn: 284.0000 - val_acc: 0.8377 - val_P: 0.0893 - val_R: 0.1744 - val_auc: 0.5397 - val_prc: 0.0731\n",
      "Epoch 42/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.1152 - tp: 3188.0000 - fp: 2616.0000 - tn: 43809.0000 - fn: 56.0000 - acc: 0.9462 - P: 0.5493 - R: 0.9827 - auc: 0.9874 - prc: 0.7956 - val_loss: 0.8538 - val_tp: 56.0000 - val_fp: 596.0000 - val_tn: 4579.0000 - val_fn: 288.0000 - val_acc: 0.8398 - val_P: 0.0859 - val_R: 0.1628 - val_auc: 0.5320 - val_prc: 0.0711\n",
      "Epoch 43/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.1100 - tp: 3193.0000 - fp: 2569.0000 - tn: 43856.0000 - fn: 51.0000 - acc: 0.9473 - P: 0.5541 - R: 0.9843 - auc: 0.9880 - prc: 0.8019 - val_loss: 0.9737 - val_tp: 32.0000 - val_fp: 388.0000 - val_tn: 4787.0000 - val_fn: 312.0000 - val_acc: 0.8732 - val_P: 0.0762 - val_R: 0.0930 - val_auc: 0.5193 - val_prc: 0.0700\n",
      "Epoch 44/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.1064 - tp: 3193.0000 - fp: 2434.0000 - tn: 43991.0000 - fn: 51.0000 - acc: 0.9500 - P: 0.5674 - R: 0.9843 - auc: 0.9885 - prc: 0.8083 - val_loss: 0.8970 - val_tp: 53.0000 - val_fp: 553.0000 - val_tn: 4622.0000 - val_fn: 291.0000 - val_acc: 0.8471 - val_P: 0.0875 - val_R: 0.1541 - val_auc: 0.5335 - val_prc: 0.0723\n",
      "Epoch 45/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.1061 - tp: 3193.0000 - fp: 2436.0000 - tn: 43989.0000 - fn: 51.0000 - acc: 0.9499 - P: 0.5672 - R: 0.9843 - auc: 0.9886 - prc: 0.8128 - val_loss: 0.8657 - val_tp: 53.0000 - val_fp: 521.0000 - val_tn: 4654.0000 - val_fn: 291.0000 - val_acc: 0.8529 - val_P: 0.0923 - val_R: 0.1541 - val_auc: 0.5327 - val_prc: 0.0727\n",
      "Epoch 46/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.1108 - tp: 3178.0000 - fp: 2539.0000 - tn: 43886.0000 - fn: 66.0000 - acc: 0.9476 - P: 0.5559 - R: 0.9797 - auc: 0.9884 - prc: 0.8159 - val_loss: 0.9305 - val_tp: 49.0000 - val_fp: 498.0000 - val_tn: 4677.0000 - val_fn: 295.0000 - val_acc: 0.8563 - val_P: 0.0896 - val_R: 0.1424 - val_auc: 0.5201 - val_prc: 0.0714\n",
      "Epoch 47/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0996 - tp: 3203.0000 - fp: 2296.0000 - tn: 44129.0000 - fn: 41.0000 - acc: 0.9529 - P: 0.5825 - R: 0.9874 - auc: 0.9895 - prc: 0.8237 - val_loss: 0.9507 - val_tp: 52.0000 - val_fp: 514.0000 - val_tn: 4661.0000 - val_fn: 292.0000 - val_acc: 0.8540 - val_P: 0.0919 - val_R: 0.1512 - val_auc: 0.5171 - val_prc: 0.0710\n",
      "Epoch 48/1000\n",
      "777/777 [==============================] - 7s 8ms/step - loss: 0.0981 - tp: 3194.0000 - fp: 2238.0000 - tn: 44187.0000 - fn: 50.0000 - acc: 0.9539 - P: 0.5880 - R: 0.9846 - auc: 0.9901 - prc: 0.8359 - val_loss: 0.9622 - val_tp: 54.0000 - val_fp: 507.0000 - val_tn: 4668.0000 - val_fn: 290.0000 - val_acc: 0.8556 - val_P: 0.0963 - val_R: 0.1570 - val_auc: 0.5195 - val_prc: 0.0733\n",
      "Epoch 49/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0999 - tp: 3186.0000 - fp: 2248.0000 - tn: 44177.0000 - fn: 58.0000 - acc: 0.9536 - P: 0.5863 - R: 0.9821 - auc: 0.9896 - prc: 0.8275 - val_loss: 1.0087 - val_tp: 65.0000 - val_fp: 715.0000 - val_tn: 4460.0000 - val_fn: 279.0000 - val_acc: 0.8199 - val_P: 0.0833 - val_R: 0.1890 - val_auc: 0.5210 - val_prc: 0.0730\n",
      "Epoch 50/1000\n",
      "777/777 [==============================] - 7s 8ms/step - loss: 0.1015 - tp: 3197.0000 - fp: 2309.0000 - tn: 44116.0000 - fn: 47.0000 - acc: 0.9526 - P: 0.5806 - R: 0.9855 - auc: 0.9898 - prc: 0.8309 - val_loss: 0.9018 - val_tp: 63.0000 - val_fp: 704.0000 - val_tn: 4471.0000 - val_fn: 281.0000 - val_acc: 0.8215 - val_P: 0.0821 - val_R: 0.1831 - val_auc: 0.5287 - val_prc: 0.0702\n",
      "Epoch 51/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0907 - tp: 3204.0000 - fp: 2102.0000 - tn: 44323.0000 - fn: 40.0000 - acc: 0.9569 - P: 0.6038 - R: 0.9877 - auc: 0.9912 - prc: 0.8451 - val_loss: 1.0023 - val_tp: 43.0000 - val_fp: 410.0000 - val_tn: 4765.0000 - val_fn: 301.0000 - val_acc: 0.8712 - val_P: 0.0949 - val_R: 0.1250 - val_auc: 0.5250 - val_prc: 0.0706\n",
      "Epoch 52/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0976 - tp: 3189.0000 - fp: 2167.0000 - tn: 44258.0000 - fn: 55.0000 - acc: 0.9553 - P: 0.5954 - R: 0.9830 - auc: 0.9903 - prc: 0.8458 - val_loss: 0.9953 - val_tp: 50.0000 - val_fp: 628.0000 - val_tn: 4547.0000 - val_fn: 294.0000 - val_acc: 0.8329 - val_P: 0.0737 - val_R: 0.1453 - val_auc: 0.5151 - val_prc: 0.0682\n",
      "Epoch 53/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0882 - tp: 3206.0000 - fp: 2009.0000 - tn: 44416.0000 - fn: 38.0000 - acc: 0.9588 - P: 0.6148 - R: 0.9883 - auc: 0.9915 - prc: 0.8553 - val_loss: 1.0426 - val_tp: 40.0000 - val_fp: 359.0000 - val_tn: 4816.0000 - val_fn: 304.0000 - val_acc: 0.8799 - val_P: 0.1003 - val_R: 0.1163 - val_auc: 0.5245 - val_prc: 0.0736\n",
      "Epoch 54/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0864 - tp: 3211.0000 - fp: 1964.0000 - tn: 44461.0000 - fn: 33.0000 - acc: 0.9598 - P: 0.6205 - R: 0.9898 - auc: 0.9918 - prc: 0.8567 - val_loss: 0.9691 - val_tp: 69.0000 - val_fp: 760.0000 - val_tn: 4415.0000 - val_fn: 275.0000 - val_acc: 0.8125 - val_P: 0.0832 - val_R: 0.2006 - val_auc: 0.5260 - val_prc: 0.0745\n",
      "Epoch 55/1000\n",
      "777/777 [==============================] - 6s 8ms/step - loss: 0.0830 - tp: 3209.0000 - fp: 1881.0000 - tn: 44544.0000 - fn: 35.0000 - acc: 0.9614 - P: 0.6305 - R: 0.9892 - auc: 0.9923 - prc: 0.8659 - val_loss: 1.0834 - val_tp: 45.0000 - val_fp: 414.0000 - val_tn: 4761.0000 - val_fn: 299.0000 - val_acc: 0.8708 - val_P: 0.0980 - val_R: 0.1308 - val_auc: 0.5092 - val_prc: 0.0690\n",
      "Epoch 56/1000\n",
      "777/777 [==============================] - 6s 8ms/step - loss: 0.0850 - tp: 3213.0000 - fp: 1905.0000 - tn: 44520.0000 - fn: 31.0000 - acc: 0.9610 - P: 0.6278 - R: 0.9904 - auc: 0.9918 - prc: 0.8587 - val_loss: 1.0285 - val_tp: 48.0000 - val_fp: 524.0000 - val_tn: 4651.0000 - val_fn: 296.0000 - val_acc: 0.8514 - val_P: 0.0839 - val_R: 0.1395 - val_auc: 0.5112 - val_prc: 0.0662\n",
      "Epoch 57/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0936 - tp: 3191.0000 - fp: 2099.0000 - tn: 44326.0000 - fn: 53.0000 - acc: 0.9567 - P: 0.6032 - R: 0.9837 - auc: 0.9914 - prc: 0.8592 - val_loss: 0.9995 - val_tp: 56.0000 - val_fp: 537.0000 - val_tn: 4638.0000 - val_fn: 288.0000 - val_acc: 0.8505 - val_P: 0.0944 - val_R: 0.1628 - val_auc: 0.5278 - val_prc: 0.0711\n",
      "Epoch 58/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0864 - tp: 3204.0000 - fp: 1990.0000 - tn: 44435.0000 - fn: 40.0000 - acc: 0.9591 - P: 0.6169 - R: 0.9877 - auc: 0.9923 - prc: 0.8681 - val_loss: 1.0261 - val_tp: 59.0000 - val_fp: 584.0000 - val_tn: 4591.0000 - val_fn: 285.0000 - val_acc: 0.8425 - val_P: 0.0918 - val_R: 0.1715 - val_auc: 0.5272 - val_prc: 0.0735\n",
      "Epoch 59/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0839 - tp: 3213.0000 - fp: 1890.0000 - tn: 44535.0000 - fn: 31.0000 - acc: 0.9613 - P: 0.6296 - R: 0.9904 - auc: 0.9925 - prc: 0.8732 - val_loss: 1.0995 - val_tp: 42.0000 - val_fp: 551.0000 - val_tn: 4624.0000 - val_fn: 302.0000 - val_acc: 0.8454 - val_P: 0.0708 - val_R: 0.1221 - val_auc: 0.5104 - val_prc: 0.0671\n",
      "Epoch 60/1000\n",
      "777/777 [==============================] - 7s 8ms/step - loss: 0.0832 - tp: 3201.0000 - fp: 1874.0000 - tn: 44551.0000 - fn: 43.0000 - acc: 0.9614 - P: 0.6307 - R: 0.9867 - auc: 0.9926 - prc: 0.8695 - val_loss: 1.0762 - val_tp: 42.0000 - val_fp: 414.0000 - val_tn: 4761.0000 - val_fn: 302.0000 - val_acc: 0.8703 - val_P: 0.0921 - val_R: 0.1221 - val_auc: 0.5112 - val_prc: 0.0673\n",
      "Epoch 61/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0769 - tp: 3211.0000 - fp: 1746.0000 - tn: 44679.0000 - fn: 33.0000 - acc: 0.9642 - P: 0.6478 - R: 0.9898 - auc: 0.9933 - prc: 0.8819 - val_loss: 1.0262 - val_tp: 44.0000 - val_fp: 457.0000 - val_tn: 4718.0000 - val_fn: 300.0000 - val_acc: 0.8628 - val_P: 0.0878 - val_R: 0.1279 - val_auc: 0.5164 - val_prc: 0.0675\n",
      "Epoch 62/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0733 - tp: 3211.0000 - fp: 1624.0000 - tn: 44801.0000 - fn: 33.0000 - acc: 0.9666 - P: 0.6641 - R: 0.9898 - auc: 0.9937 - prc: 0.8876 - val_loss: 1.0592 - val_tp: 46.0000 - val_fp: 528.0000 - val_tn: 4647.0000 - val_fn: 298.0000 - val_acc: 0.8503 - val_P: 0.0801 - val_R: 0.1337 - val_auc: 0.5125 - val_prc: 0.0693\n",
      "Epoch 63/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0839 - tp: 3196.0000 - fp: 1865.0000 - tn: 44560.0000 - fn: 48.0000 - acc: 0.9615 - P: 0.6315 - R: 0.9852 - auc: 0.9930 - prc: 0.8825 - val_loss: 1.1554 - val_tp: 68.0000 - val_fp: 771.0000 - val_tn: 4404.0000 - val_fn: 276.0000 - val_acc: 0.8103 - val_P: 0.0810 - val_R: 0.1977 - val_auc: 0.5057 - val_prc: 0.0692\n",
      "Epoch 64/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0735 - tp: 3208.0000 - fp: 1668.0000 - tn: 44757.0000 - fn: 36.0000 - acc: 0.9657 - P: 0.6579 - R: 0.9889 - auc: 0.9940 - prc: 0.8926 - val_loss: 1.0611 - val_tp: 54.0000 - val_fp: 523.0000 - val_tn: 4652.0000 - val_fn: 290.0000 - val_acc: 0.8527 - val_P: 0.0936 - val_R: 0.1570 - val_auc: 0.5199 - val_prc: 0.0696\n",
      "Epoch 65/1000\n",
      "777/777 [==============================] - 6s 7ms/step - loss: 0.0748 - tp: 3207.0000 - fp: 1658.0000 - tn: 44767.0000 - fn: 37.0000 - acc: 0.9659 - P: 0.6592 - R: 0.9886 - auc: 0.9940 - prc: 0.9002 - val_loss: 1.1199 - val_tp: 65.0000 - val_fp: 735.0000 - val_tn: 4440.0000 - val_fn: 279.0000 - val_acc: 0.8163 - val_P: 0.0812 - val_R: 0.1890 - val_auc: 0.5144 - val_prc: 0.0689\n",
      "Epoch 66/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0724 - tp: 3211.0000 - fp: 1644.0000 - tn: 44781.0000 - fn: 33.0000 - acc: 0.9662 - P: 0.6614 - R: 0.9898 - auc: 0.9941 - prc: 0.8920 - val_loss: 1.1419 - val_tp: 44.0000 - val_fp: 495.0000 - val_tn: 4680.0000 - val_fn: 300.0000 - val_acc: 0.8560 - val_P: 0.0816 - val_R: 0.1279 - val_auc: 0.5125 - val_prc: 0.0666\n",
      "Epoch 67/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0729 - tp: 3219.0000 - fp: 1572.0000 - tn: 44853.0000 - fn: 25.0000 - acc: 0.9678 - P: 0.6719 - R: 0.9923 - auc: 0.9939 - prc: 0.8929 - val_loss: 1.1861 - val_tp: 38.0000 - val_fp: 372.0000 - val_tn: 4803.0000 - val_fn: 306.0000 - val_acc: 0.8772 - val_P: 0.0927 - val_R: 0.1105 - val_auc: 0.5139 - val_prc: 0.0685\n",
      "Epoch 68/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0770 - tp: 3201.0000 - fp: 1666.0000 - tn: 44759.0000 - fn: 43.0000 - acc: 0.9656 - P: 0.6577 - R: 0.9867 - auc: 0.9938 - prc: 0.8899 - val_loss: 1.1888 - val_tp: 33.0000 - val_fp: 325.0000 - val_tn: 4850.0000 - val_fn: 311.0000 - val_acc: 0.8848 - val_P: 0.0922 - val_R: 0.0959 - val_auc: 0.5100 - val_prc: 0.0681\n",
      "Epoch 69/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0673 - tp: 3217.0000 - fp: 1505.0000 - tn: 44920.0000 - fn: 27.0000 - acc: 0.9692 - P: 0.6813 - R: 0.9917 - auc: 0.9946 - prc: 0.9008 - val_loss: 1.2001 - val_tp: 39.0000 - val_fp: 385.0000 - val_tn: 4790.0000 - val_fn: 305.0000 - val_acc: 0.8750 - val_P: 0.0920 - val_R: 0.1134 - val_auc: 0.5272 - val_prc: 0.0712\n",
      "Epoch 70/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0684 - tp: 3209.0000 - fp: 1463.0000 - tn: 44962.0000 - fn: 35.0000 - acc: 0.9698 - P: 0.6869 - R: 0.9892 - auc: 0.9945 - prc: 0.9012 - val_loss: 1.3321 - val_tp: 38.0000 - val_fp: 543.0000 - val_tn: 4632.0000 - val_fn: 306.0000 - val_acc: 0.8462 - val_P: 0.0654 - val_R: 0.1105 - val_auc: 0.5132 - val_prc: 0.0664\n",
      "Epoch 71/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0663 - tp: 3216.0000 - fp: 1480.0000 - tn: 44945.0000 - fn: 28.0000 - acc: 0.9696 - P: 0.6848 - R: 0.9914 - auc: 0.9947 - prc: 0.9085 - val_loss: 1.1908 - val_tp: 41.0000 - val_fp: 417.0000 - val_tn: 4758.0000 - val_fn: 303.0000 - val_acc: 0.8695 - val_P: 0.0895 - val_R: 0.1192 - val_auc: 0.5210 - val_prc: 0.0674\n",
      "Epoch 72/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0610 - tp: 3216.0000 - fp: 1388.0000 - tn: 45037.0000 - fn: 28.0000 - acc: 0.9715 - P: 0.6985 - R: 0.9914 - auc: 0.9955 - prc: 0.9132 - val_loss: 1.1737 - val_tp: 46.0000 - val_fp: 521.0000 - val_tn: 4654.0000 - val_fn: 298.0000 - val_acc: 0.8516 - val_P: 0.0811 - val_R: 0.1337 - val_auc: 0.5241 - val_prc: 0.0680\n",
      "Epoch 73/1000\n",
      "777/777 [==============================] - 6s 8ms/step - loss: 0.0697 - tp: 3212.0000 - fp: 1524.0000 - tn: 44901.0000 - fn: 32.0000 - acc: 0.9687 - P: 0.6782 - R: 0.9901 - auc: 0.9946 - prc: 0.9044 - val_loss: 1.1006 - val_tp: 60.0000 - val_fp: 650.0000 - val_tn: 4525.0000 - val_fn: 284.0000 - val_acc: 0.8308 - val_P: 0.0845 - val_R: 0.1744 - val_auc: 0.5255 - val_prc: 0.0698\n",
      "Epoch 74/1000\n",
      "777/777 [==============================] - 7s 8ms/step - loss: 0.0818 - tp: 3205.0000 - fp: 1760.0000 - tn: 44665.0000 - fn: 39.0000 - acc: 0.9638 - P: 0.6455 - R: 0.9880 - auc: 0.9929 - prc: 0.8759 - val_loss: 1.1808 - val_tp: 47.0000 - val_fp: 500.0000 - val_tn: 4675.0000 - val_fn: 297.0000 - val_acc: 0.8556 - val_P: 0.0859 - val_R: 0.1366 - val_auc: 0.5124 - val_prc: 0.0694\n",
      "Epoch 75/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0513 - tp: 3235.0000 - fp: 1171.0000 - tn: 45254.0000 - fn: 9.0000 - acc: 0.9762 - P: 0.7342 - R: 0.9972 - auc: 0.9963 - prc: 0.9289 - val_loss: 1.1702 - val_tp: 44.0000 - val_fp: 460.0000 - val_tn: 4715.0000 - val_fn: 300.0000 - val_acc: 0.8623 - val_P: 0.0873 - val_R: 0.1279 - val_auc: 0.5194 - val_prc: 0.0689\n",
      "Epoch 76/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0756 - tp: 3201.0000 - fp: 1623.0000 - tn: 44802.0000 - fn: 43.0000 - acc: 0.9665 - P: 0.6636 - R: 0.9867 - auc: 0.9940 - prc: 0.8949 - val_loss: 1.1846 - val_tp: 40.0000 - val_fp: 462.0000 - val_tn: 4713.0000 - val_fn: 304.0000 - val_acc: 0.8612 - val_P: 0.0797 - val_R: 0.1163 - val_auc: 0.5071 - val_prc: 0.0659\n",
      "Epoch 77/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0713 - tp: 3211.0000 - fp: 1612.0000 - tn: 44813.0000 - fn: 33.0000 - acc: 0.9669 - P: 0.6658 - R: 0.9898 - auc: 0.9948 - prc: 0.9077 - val_loss: 1.2022 - val_tp: 34.0000 - val_fp: 416.0000 - val_tn: 4759.0000 - val_fn: 310.0000 - val_acc: 0.8685 - val_P: 0.0756 - val_R: 0.0988 - val_auc: 0.5236 - val_prc: 0.0680\n",
      "Epoch 78/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0596 - tp: 3216.0000 - fp: 1322.0000 - tn: 45103.0000 - fn: 28.0000 - acc: 0.9728 - P: 0.7087 - R: 0.9914 - auc: 0.9958 - prc: 0.9231 - val_loss: 1.1613 - val_tp: 59.0000 - val_fp: 547.0000 - val_tn: 4628.0000 - val_fn: 285.0000 - val_acc: 0.8492 - val_P: 0.0974 - val_R: 0.1715 - val_auc: 0.5263 - val_prc: 0.0702\n",
      "Epoch 79/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0641 - tp: 3217.0000 - fp: 1416.0000 - tn: 45009.0000 - fn: 27.0000 - acc: 0.9709 - P: 0.6944 - R: 0.9917 - auc: 0.9952 - prc: 0.9130 - val_loss: 1.2304 - val_tp: 46.0000 - val_fp: 438.0000 - val_tn: 4737.0000 - val_fn: 298.0000 - val_acc: 0.8666 - val_P: 0.0950 - val_R: 0.1337 - val_auc: 0.5236 - val_prc: 0.0704\n",
      "Epoch 80/1000\n",
      "777/777 [==============================] - 7s 8ms/step - loss: 0.0542 - tp: 3224.0000 - fp: 1158.0000 - tn: 45267.0000 - fn: 20.0000 - acc: 0.9763 - P: 0.7357 - R: 0.9938 - auc: 0.9962 - prc: 0.9282 - val_loss: 1.1611 - val_tp: 60.0000 - val_fp: 558.0000 - val_tn: 4617.0000 - val_fn: 284.0000 - val_acc: 0.8474 - val_P: 0.0971 - val_R: 0.1744 - val_auc: 0.5269 - val_prc: 0.0736\n",
      "Epoch 81/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0626 - tp: 3215.0000 - fp: 1412.0000 - tn: 45013.0000 - fn: 29.0000 - acc: 0.9710 - P: 0.6948 - R: 0.9911 - auc: 0.9955 - prc: 0.9166 - val_loss: 1.3312 - val_tp: 33.0000 - val_fp: 361.0000 - val_tn: 4814.0000 - val_fn: 311.0000 - val_acc: 0.8782 - val_P: 0.0838 - val_R: 0.0959 - val_auc: 0.5228 - val_prc: 0.0706\n",
      "Epoch 82/1000\n",
      "777/777 [==============================] - 7s 8ms/step - loss: 0.0506 - tp: 3223.0000 - fp: 1128.0000 - tn: 45297.0000 - fn: 21.0000 - acc: 0.9769 - P: 0.7407 - R: 0.9935 - auc: 0.9965 - prc: 0.9312 - val_loss: 1.3051 - val_tp: 39.0000 - val_fp: 449.0000 - val_tn: 4726.0000 - val_fn: 305.0000 - val_acc: 0.8634 - val_P: 0.0799 - val_R: 0.1134 - val_auc: 0.5208 - val_prc: 0.0690\n",
      "Epoch 83/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0509 - tp: 3230.0000 - fp: 1128.0000 - tn: 45297.0000 - fn: 14.0000 - acc: 0.9770 - P: 0.7412 - R: 0.9957 - auc: 0.9963 - prc: 0.9324 - val_loss: 1.3679 - val_tp: 48.0000 - val_fp: 526.0000 - val_tn: 4649.0000 - val_fn: 296.0000 - val_acc: 0.8511 - val_P: 0.0836 - val_R: 0.1395 - val_auc: 0.5190 - val_prc: 0.0693\n",
      "Epoch 84/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0641 - tp: 3213.0000 - fp: 1362.0000 - tn: 45063.0000 - fn: 31.0000 - acc: 0.9720 - P: 0.7023 - R: 0.9904 - auc: 0.9951 - prc: 0.9107 - val_loss: 1.3800 - val_tp: 35.0000 - val_fp: 379.0000 - val_tn: 4796.0000 - val_fn: 309.0000 - val_acc: 0.8753 - val_P: 0.0845 - val_R: 0.1017 - val_auc: 0.5146 - val_prc: 0.0674\n",
      "Epoch 85/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0577 - tp: 3215.0000 - fp: 1278.0000 - tn: 45147.0000 - fn: 29.0000 - acc: 0.9737 - P: 0.7156 - R: 0.9911 - auc: 0.9960 - prc: 0.9223 - val_loss: 1.3909 - val_tp: 38.0000 - val_fp: 551.0000 - val_tn: 4624.0000 - val_fn: 306.0000 - val_acc: 0.8447 - val_P: 0.0645 - val_R: 0.1105 - val_auc: 0.4946 - val_prc: 0.0620\n",
      "Epoch 86/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0646 - tp: 3212.0000 - fp: 1379.0000 - tn: 45046.0000 - fn: 32.0000 - acc: 0.9716 - P: 0.6996 - R: 0.9901 - auc: 0.9951 - prc: 0.9143 - val_loss: 1.2510 - val_tp: 48.0000 - val_fp: 504.0000 - val_tn: 4671.0000 - val_fn: 296.0000 - val_acc: 0.8550 - val_P: 0.0870 - val_R: 0.1395 - val_auc: 0.5292 - val_prc: 0.0713\n",
      "Epoch 87/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0545 - tp: 3221.0000 - fp: 1195.0000 - tn: 45230.0000 - fn: 23.0000 - acc: 0.9755 - P: 0.7294 - R: 0.9929 - auc: 0.9961 - prc: 0.9317 - val_loss: 1.2489 - val_tp: 39.0000 - val_fp: 434.0000 - val_tn: 4741.0000 - val_fn: 305.0000 - val_acc: 0.8661 - val_P: 0.0825 - val_R: 0.1134 - val_auc: 0.5271 - val_prc: 0.0692\n",
      "Epoch 88/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0526 - tp: 3226.0000 - fp: 1135.0000 - tn: 45290.0000 - fn: 18.0000 - acc: 0.9768 - P: 0.7397 - R: 0.9945 - auc: 0.9964 - prc: 0.9334 - val_loss: 1.2569 - val_tp: 31.0000 - val_fp: 372.0000 - val_tn: 4803.0000 - val_fn: 313.0000 - val_acc: 0.8759 - val_P: 0.0769 - val_R: 0.0901 - val_auc: 0.5189 - val_prc: 0.0673\n",
      "Epoch 89/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0457 - tp: 3234.0000 - fp: 986.0000 - tn: 45439.0000 - fn: 10.0000 - acc: 0.9799 - P: 0.7664 - R: 0.9969 - auc: 0.9968 - prc: 0.9372 - val_loss: 1.3529 - val_tp: 37.0000 - val_fp: 417.0000 - val_tn: 4758.0000 - val_fn: 307.0000 - val_acc: 0.8688 - val_P: 0.0815 - val_R: 0.1076 - val_auc: 0.5185 - val_prc: 0.0680\n",
      "Epoch 90/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0515 - tp: 3224.0000 - fp: 1076.0000 - tn: 45349.0000 - fn: 20.0000 - acc: 0.9779 - P: 0.7498 - R: 0.9938 - auc: 0.9967 - prc: 0.9339 - val_loss: 1.2006 - val_tp: 41.0000 - val_fp: 532.0000 - val_tn: 4643.0000 - val_fn: 303.0000 - val_acc: 0.8487 - val_P: 0.0716 - val_R: 0.1192 - val_auc: 0.5324 - val_prc: 0.0697\n",
      "Epoch 91/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0555 - tp: 3218.0000 - fp: 1245.0000 - tn: 45180.0000 - fn: 26.0000 - acc: 0.9744 - P: 0.7210 - R: 0.9920 - auc: 0.9964 - prc: 0.9328 - val_loss: 1.3647 - val_tp: 36.0000 - val_fp: 364.0000 - val_tn: 4811.0000 - val_fn: 308.0000 - val_acc: 0.8782 - val_P: 0.0900 - val_R: 0.1047 - val_auc: 0.5297 - val_prc: 0.0705\n",
      "Epoch 92/1000\n",
      "777/777 [==============================] - 7s 8ms/step - loss: 0.0561 - tp: 3213.0000 - fp: 1229.0000 - tn: 45196.0000 - fn: 31.0000 - acc: 0.9746 - P: 0.7233 - R: 0.9904 - auc: 0.9966 - prc: 0.9386 - val_loss: 1.2790 - val_tp: 35.0000 - val_fp: 368.0000 - val_tn: 4807.0000 - val_fn: 309.0000 - val_acc: 0.8773 - val_P: 0.0868 - val_R: 0.1017 - val_auc: 0.5183 - val_prc: 0.0682\n",
      "Epoch 93/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0443 - tp: 3228.0000 - fp: 941.0000 - tn: 45484.0000 - fn: 16.0000 - acc: 0.9807 - P: 0.7743 - R: 0.9951 - auc: 0.9972 - prc: 0.9441 - val_loss: 1.4427 - val_tp: 34.0000 - val_fp: 389.0000 - val_tn: 4786.0000 - val_fn: 310.0000 - val_acc: 0.8733 - val_P: 0.0804 - val_R: 0.0988 - val_auc: 0.5259 - val_prc: 0.0713\n",
      "Epoch 94/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0540 - tp: 3217.0000 - fp: 1171.0000 - tn: 45254.0000 - fn: 27.0000 - acc: 0.9759 - P: 0.7331 - R: 0.9917 - auc: 0.9964 - prc: 0.9355 - val_loss: 1.3643 - val_tp: 31.0000 - val_fp: 429.0000 - val_tn: 4746.0000 - val_fn: 313.0000 - val_acc: 0.8656 - val_P: 0.0674 - val_R: 0.0901 - val_auc: 0.5245 - val_prc: 0.0677\n",
      "Epoch 95/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0506 - tp: 3222.0000 - fp: 1071.0000 - tn: 45354.0000 - fn: 22.0000 - acc: 0.9780 - P: 0.7505 - R: 0.9932 - auc: 0.9965 - prc: 0.9322 - val_loss: 1.2424 - val_tp: 39.0000 - val_fp: 428.0000 - val_tn: 4747.0000 - val_fn: 305.0000 - val_acc: 0.8672 - val_P: 0.0835 - val_R: 0.1134 - val_auc: 0.5283 - val_prc: 0.0694\n",
      "Epoch 96/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0382 - tp: 3237.0000 - fp: 862.0000 - tn: 45563.0000 - fn: 7.0000 - acc: 0.9825 - P: 0.7897 - R: 0.9978 - auc: 0.9978 - prc: 0.9554 - val_loss: 1.3332 - val_tp: 40.0000 - val_fp: 414.0000 - val_tn: 4761.0000 - val_fn: 304.0000 - val_acc: 0.8699 - val_P: 0.0881 - val_R: 0.1163 - val_auc: 0.5100 - val_prc: 0.0661\n",
      "Epoch 97/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0488 - tp: 3225.0000 - fp: 1060.0000 - tn: 45365.0000 - fn: 19.0000 - acc: 0.9783 - P: 0.7526 - R: 0.9941 - auc: 0.9969 - prc: 0.9388 - val_loss: 1.2712 - val_tp: 39.0000 - val_fp: 409.0000 - val_tn: 4766.0000 - val_fn: 305.0000 - val_acc: 0.8706 - val_P: 0.0871 - val_R: 0.1134 - val_auc: 0.5155 - val_prc: 0.0665\n",
      "Epoch 98/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0499 - tp: 3223.0000 - fp: 1080.0000 - tn: 45345.0000 - fn: 21.0000 - acc: 0.9778 - P: 0.7490 - R: 0.9935 - auc: 0.9968 - prc: 0.9363 - val_loss: 1.3877 - val_tp: 37.0000 - val_fp: 445.0000 - val_tn: 4730.0000 - val_fn: 307.0000 - val_acc: 0.8637 - val_P: 0.0768 - val_R: 0.1076 - val_auc: 0.5129 - val_prc: 0.0668\n",
      "Epoch 99/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0369 - tp: 3233.0000 - fp: 804.0000 - tn: 45621.0000 - fn: 11.0000 - acc: 0.9836 - P: 0.8008 - R: 0.9966 - auc: 0.9977 - prc: 0.9524 - val_loss: 1.4869 - val_tp: 34.0000 - val_fp: 363.0000 - val_tn: 4812.0000 - val_fn: 310.0000 - val_acc: 0.8781 - val_P: 0.0856 - val_R: 0.0988 - val_auc: 0.5176 - val_prc: 0.0682\n",
      "Epoch 100/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0436 - tp: 3228.0000 - fp: 939.0000 - tn: 45486.0000 - fn: 16.0000 - acc: 0.9808 - P: 0.7747 - R: 0.9951 - auc: 0.9972 - prc: 0.9464 - val_loss: 1.5643 - val_tp: 35.0000 - val_fp: 329.0000 - val_tn: 4846.0000 - val_fn: 309.0000 - val_acc: 0.8844 - val_P: 0.0962 - val_R: 0.1017 - val_auc: 0.5140 - val_prc: 0.0667\n",
      "Epoch 101/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0475 - tp: 3221.0000 - fp: 994.0000 - tn: 45431.0000 - fn: 23.0000 - acc: 0.9795 - P: 0.7642 - R: 0.9929 - auc: 0.9972 - prc: 0.9451 - val_loss: 1.5297 - val_tp: 47.0000 - val_fp: 454.0000 - val_tn: 4721.0000 - val_fn: 297.0000 - val_acc: 0.8639 - val_P: 0.0938 - val_R: 0.1366 - val_auc: 0.5246 - val_prc: 0.0731\n",
      "Epoch 102/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0562 - tp: 3209.0000 - fp: 1199.0000 - tn: 45226.0000 - fn: 35.0000 - acc: 0.9752 - P: 0.7280 - R: 0.9892 - auc: 0.9963 - prc: 0.9316 - val_loss: 1.4098 - val_tp: 36.0000 - val_fp: 380.0000 - val_tn: 4795.0000 - val_fn: 308.0000 - val_acc: 0.8753 - val_P: 0.0865 - val_R: 0.1047 - val_auc: 0.5140 - val_prc: 0.0675\n",
      "Epoch 103/1000\n",
      "777/777 [==============================] - 7s 8ms/step - loss: 0.0365 - tp: 3235.0000 - fp: 819.0000 - tn: 45606.0000 - fn: 9.0000 - acc: 0.9833 - P: 0.7980 - R: 0.9972 - auc: 0.9980 - prc: 0.9600 - val_loss: 1.4815 - val_tp: 34.0000 - val_fp: 379.0000 - val_tn: 4796.0000 - val_fn: 310.0000 - val_acc: 0.8752 - val_P: 0.0823 - val_R: 0.0988 - val_auc: 0.5213 - val_prc: 0.0686\n",
      "Epoch 104/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0379 - tp: 3232.0000 - fp: 822.0000 - tn: 45603.0000 - fn: 12.0000 - acc: 0.9832 - P: 0.7972 - R: 0.9963 - auc: 0.9978 - prc: 0.9551 - val_loss: 1.4662 - val_tp: 32.0000 - val_fp: 396.0000 - val_tn: 4779.0000 - val_fn: 312.0000 - val_acc: 0.8717 - val_P: 0.0748 - val_R: 0.0930 - val_auc: 0.5166 - val_prc: 0.0678\n",
      "Epoch 105/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0466 - tp: 3219.0000 - fp: 997.0000 - tn: 45428.0000 - fn: 25.0000 - acc: 0.9794 - P: 0.7635 - R: 0.9923 - auc: 0.9972 - prc: 0.9430 - val_loss: 1.4615 - val_tp: 31.0000 - val_fp: 359.0000 - val_tn: 4816.0000 - val_fn: 313.0000 - val_acc: 0.8782 - val_P: 0.0795 - val_R: 0.0901 - val_auc: 0.5205 - val_prc: 0.0694\n",
      "Epoch 106/1000\n",
      "777/777 [==============================] - 7s 8ms/step - loss: 0.0456 - tp: 3218.0000 - fp: 959.0000 - tn: 45466.0000 - fn: 26.0000 - acc: 0.9802 - P: 0.7704 - R: 0.9920 - auc: 0.9972 - prc: 0.9468 - val_loss: 1.3175 - val_tp: 55.0000 - val_fp: 576.0000 - val_tn: 4599.0000 - val_fn: 289.0000 - val_acc: 0.8433 - val_P: 0.0872 - val_R: 0.1599 - val_auc: 0.5257 - val_prc: 0.0682\n",
      "Epoch 107/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0384 - tp: 3231.0000 - fp: 848.0000 - tn: 45577.0000 - fn: 13.0000 - acc: 0.9827 - P: 0.7921 - R: 0.9960 - auc: 0.9979 - prc: 0.9587 - val_loss: 1.4391 - val_tp: 46.0000 - val_fp: 467.0000 - val_tn: 4708.0000 - val_fn: 298.0000 - val_acc: 0.8614 - val_P: 0.0897 - val_R: 0.1337 - val_auc: 0.5275 - val_prc: 0.0702\n",
      "Epoch 108/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0425 - tp: 3220.0000 - fp: 920.0000 - tn: 45505.0000 - fn: 24.0000 - acc: 0.9810 - P: 0.7778 - R: 0.9926 - auc: 0.9976 - prc: 0.9502 - val_loss: 1.4649 - val_tp: 37.0000 - val_fp: 374.0000 - val_tn: 4801.0000 - val_fn: 307.0000 - val_acc: 0.8766 - val_P: 0.0900 - val_R: 0.1076 - val_auc: 0.5315 - val_prc: 0.0718\n",
      "Epoch 109/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0373 - tp: 3230.0000 - fp: 802.0000 - tn: 45623.0000 - fn: 14.0000 - acc: 0.9836 - P: 0.8011 - R: 0.9957 - auc: 0.9979 - prc: 0.9572 - val_loss: 1.5011 - val_tp: 29.0000 - val_fp: 355.0000 - val_tn: 4820.0000 - val_fn: 315.0000 - val_acc: 0.8786 - val_P: 0.0755 - val_R: 0.0843 - val_auc: 0.5166 - val_prc: 0.0665\n",
      "Epoch 110/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0448 - tp: 3222.0000 - fp: 921.0000 - tn: 45504.0000 - fn: 22.0000 - acc: 0.9810 - P: 0.7777 - R: 0.9932 - auc: 0.9973 - prc: 0.9432 - val_loss: 1.5521 - val_tp: 33.0000 - val_fp: 352.0000 - val_tn: 4823.0000 - val_fn: 311.0000 - val_acc: 0.8799 - val_P: 0.0857 - val_R: 0.0959 - val_auc: 0.5178 - val_prc: 0.0685\n",
      "Epoch 111/1000\n",
      "777/777 [==============================] - 7s 9ms/step - loss: 0.0461 - tp: 3222.0000 - fp: 961.0000 - tn: 45464.0000 - fn: 22.0000 - acc: 0.9802 - P: 0.7703 - R: 0.9932 - auc: 0.9973 - prc: 0.9442 - val_loss: 1.4988 - val_tp: 28.0000 - val_fp: 373.0000 - val_tn: 4802.0000 - val_fn: 316.0000 - val_acc: 0.8752 - val_P: 0.0698 - val_R: 0.0814 - val_auc: 0.5179 - val_prc: 0.0665\n",
      "Epoch 112/1000\n",
      "353/777 [============>.................] - ETA: 3s - loss: 0.0264 - tp: 1481.0000 - fp: 267.0000 - tn: 20843.0000 - fn: 1.0000 - acc: 0.9881 - P: 0.8473 - R: 0.9993 - auc: 0.9988 - prc: 0.9738"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misX_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([sX_train, pX_train, isX_train], y_train, epochs=1000, batch_size=64, validation_split=0.1, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_64\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_124 (InputLayer)      [(None, 1, 1536)]            0         []                            \n",
      "                                                                                                  \n",
      " input_123 (InputLayer)      [(None, 15, 1536)]           0         []                            \n",
      "                                                                                                  \n",
      " attention (MultiHeadAttent  (None, 1, 1536)              1384611   ['input_124[0][0]',           \n",
      " ion)                                                                'input_123[0][0]']           \n",
      "                                                                                                  \n",
      " layer_normalization_114 (L  (None, 1, 1536)              3072      ['attention[0][0]']           \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " sequential_63 (Sequential)  (None, 1, 1536)              6295040   ['layer_normalization_114[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_64 (Add)                (None, 1, 1536)              0         ['layer_normalization_114[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'sequential_63[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_115 (L  (None, 1, 1536)              3072      ['add_64[0][0]']              \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " flatten_48 (Flatten)        (None, 1536)                 0         ['layer_normalization_115[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dense_235 (Dense)           (None, 1536)                 2360832   ['flatten_48[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10046627 (38.32 MB)\n",
      "Trainable params: 10046627 (38.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# generative separate inputs\n",
    "\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "suggested_tensor = tf.keras.Input(shape=[15, 1536])\n",
    "playing_tensor = tf.keras.Input(shape=[1, 1536])\n",
    "layer = MultiHeadAttention(num_heads=15, key_dim=15, attention_axes=(1), name='attention')\n",
    "x = layer(playing_tensor, suggested_tensor)\n",
    "# x = layer(suggested_tensor, suggested_tensor)\n",
    "layer_norm = tf.keras.layers.LayerNormalization()\n",
    "x = layer_norm(x)\n",
    "\n",
    "d_model = 1536\n",
    "\n",
    "seq = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(2048, activation='relu'),\n",
    "    tf.keras.layers.Dense(d_model, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1)\n",
    "])\n",
    "add = tf.keras.layers.Add()\n",
    "x = add([x, seq(x)])\n",
    "layer_norm = tf.keras.layers.LayerNormalization()\n",
    "x = layer_norm(x)\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "final = tf.keras.layers.Dense(1536, activation='tanh')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[suggested_tensor, playing_tensor], outputs=final)\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['cosine_similarity'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query (1536, 15, 15)\n",
      "keys (15, 15)\n",
      "values (1536, 15, 15)\n",
      "proj (15, 15)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = model.get_layer('attention')\n",
    "weight_names = ['query', 'keys',  'values', 'proj']\n",
    "for name, out in zip(weight_names,layer.get_weights()):\n",
    "    print(name, out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = np.array(Y)\n",
    "# pX = pX.reshape((pX.shape[0], 1,pX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit([sX_train, pX_train], y_train, epochs=1000, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 14ms/step - loss: 3.1438e-04 - cosine_similarity: 0.7496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.00031437957659363747, 0.7495620846748352]"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([sX_test, pX_test], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.argsort([1, 2, 3, 4, 99, 6, 44, 0])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6, 5, 3, 2, 1, 0, 7])"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  5,  9])"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tp = 0\n",
    "fp = 0\n",
    "\n",
    "\n",
    "pred = model.predict([sX_test, pX_test])\n",
    "\n",
    "I = []\n",
    "J = []\n",
    "\n",
    "for ii in range(0, 17):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for i in range(100):\n",
    "        p = np.argsort(np.array(cosine_similarity(sX_test[i], [pred[i]])).reshape(15))[::-1]\n",
    "        p = p[:ii]\n",
    "        y = np.argmax(cosine_similarity(sX_test[i], [y_test[i]]))\n",
    "        if y in p:\n",
    "            tp += 1\n",
    "        fp += 1\n",
    "\n",
    "    I.append(ii)\n",
    "    J.append(tp/fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y in p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.09,\n",
       " 0.17,\n",
       " 0.21,\n",
       " 0.32,\n",
       " 0.36,\n",
       " 0.42,\n",
       " 0.48,\n",
       " 0.56,\n",
       " 0.61,\n",
       " 0.7,\n",
       " 0.79,\n",
       " 0.84,\n",
       " 0.92,\n",
       " 0.93,\n",
       " 1.0,\n",
       " 1.0]"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fb4a4b83460>"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(I, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_54\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_104 (InputLayer)      [(None, 1, 1536)]            0         []                            \n",
      "                                                                                                  \n",
      " input_103 (InputLayer)      [(None, 15, 1536)]           0         []                            \n",
      "                                                                                                  \n",
      " multi_head_attention_71 (M  (None, 1, 1536)              155211    ['input_104[0][0]',           \n",
      " ultiHeadAttention)                                                  'input_103[0][0]']           \n",
      "                                                                                                  \n",
      " layer_normalization_98 (La  (None, 1, 1536)              3072      ['multi_head_attention_71[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " sequential_55 (Sequential)  (None, 1, 1536)              6295040   ['layer_normalization_98[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_56 (Add)                (None, 1, 1536)              0         ['layer_normalization_98[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'sequential_55[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_99 (La  (None, 1, 1536)              3072      ['add_56[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " flatten_40 (Flatten)        (None, 1536)                 0         ['layer_normalization_99[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_210 (Dense)           (None, 1536)                 2360832   ['flatten_40[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8817227 (33.64 MB)\n",
      "Trainable params: 8817227 (33.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query (1536, 15, 1)\n",
      "keys (15, 1)\n",
      "values (1536, 15, 1)\n",
      "proj (15, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = model.get_layer('attention')\n",
    "weight_names = ['query', 'keys',  'values', 'proj']\n",
    "for name, out in zip(weight_names,layer.get_weights()):\n",
    "    print(name, out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = tf.keras.Model(inputs=model.input, \n",
    "                                 outputs=model.get_layer(\"attention\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/138 [..............................] - ETA: 3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsmhabib/.conda/envs/ytbase/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "yy = attention.predict([sX, pX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4413, 1, 1536)"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "suggested_tensor = tf.keras.Input(shape=[15, 1536])\n",
    "playing_tensor = tf.keras.Input(shape=[1, 1536])\n",
    "layer = MultiHeadAttention(num_heads=5, key_dim=5)\n",
    "x = layer(suggested_tensor, playing_tensor)\n",
    "layer_norm = tf.keras.layers.LayerNormalization()\n",
    "x = layer_norm(x)\n",
    "\n",
    "d_model = 1536\n",
    "\n",
    "seq = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(2048, activation='relu'),\n",
    "    tf.keras.layers.Dense(d_model, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1)\n",
    "])\n",
    "add = tf.keras.layers.Add()\n",
    "x = add([x, seq(x)])\n",
    "layer_norm = tf.keras.layers.LayerNormalization()\n",
    "x = layer_norm(x)\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "final = tf.keras.layers.Dense(1536, activation='relu')(x)\n",
    "final = tf.keras.layers.Dense(1536, activation='relu')(x)\n",
    "final = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "final = tf.keras.layers.Dense(15, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[suggested_tensor, playing_tensor], outputs=final)\n",
    "model.compile(loss='cosine_similarity', optimizer='adam', metrics=['cosine_similarity'])\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
