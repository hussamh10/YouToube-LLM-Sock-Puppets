{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 13:12:41.793030: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-17 13:12:42.459664: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 13:13:01.478597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46692 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-08-17 13:13:01.479494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46692 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('../../data/datasets/raw-video-level-watches')\n",
    "videos = pd.read_pickle('../../data/videos_raw_metadata')\n",
    "title_embeddings = pd.read_pickle('../../data/embeddings/title-autoencoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(vec, element, length=15):\n",
    "    #  a vector to length and keep the element\n",
    "    if len(vec) > length:\n",
    "        vec = vec[:length]\n",
    "    if element not in vec:\n",
    "        vec = vec[:-1] + [element]\n",
    "\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = []\n",
    "positive = []\n",
    "negative = []\n",
    "\n",
    "for watch in df_train:\n",
    "    playing = watch['playing']\n",
    "    selected = watch['selected']\n",
    "    suggesteds = watch['upnext']\n",
    "\n",
    "    suggesteds = trim(suggesteds, selected)\n",
    "    \n",
    "    if playing not in videos or selected not in videos or any([s not in videos for s in suggesteds]):\n",
    "        continue\n",
    "    \n",
    "    p = np.array(title_embeddings[videos[playing]['snippet']['title']])[0]\n",
    "    sv = np.array(title_embeddings[videos[selected]['snippet']['title']])[0]\n",
    "    sx = [title_embeddings[videos[s]['snippet']['title']] for s in suggesteds]\n",
    "    smean = np.mean(np.array(sx), axis=0)[0]\n",
    "    nxs = []\n",
    "    px = []\n",
    "    ax = [p, smean]\n",
    "\n",
    "    for s, sxx in zip(suggesteds, sx):\n",
    "        if s == selected:\n",
    "            px = [sv, smean]\n",
    "        else:\n",
    "            sxx = np.array(sxx)[0]\n",
    "            nx = [sxx, smean]\n",
    "            nxs.append(nx)\n",
    "        \n",
    "    if px == []:\n",
    "        continue\n",
    "\n",
    "    for nx in nxs:\n",
    "        anchor.append(ax)\n",
    "        positive.append(px)\n",
    "        negative.append(nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_test = []\n",
    "positive_test = []\n",
    "negative_test = []\n",
    "\n",
    "for watch in df_test:\n",
    "    playing = watch['playing']\n",
    "    selected = watch['selected']\n",
    "    suggesteds = watch['upnext']\n",
    "\n",
    "    suggesteds = trim(suggesteds, selected)\n",
    "    \n",
    "    if playing not in videos or selected not in videos or any([s not in videos for s in suggesteds]):\n",
    "        continue\n",
    "    \n",
    "    p = np.array(title_embeddings[videos[playing]['snippet']['title']])[0]\n",
    "    sv = np.array(title_embeddings[videos[selected]['snippet']['title']])[0]\n",
    "    sx = [title_embeddings[videos[s]['snippet']['title']] for s in suggesteds]\n",
    "    smean = np.mean(np.array(sx), axis=0)[0]\n",
    "    nxs = []\n",
    "    px = []\n",
    "    ax = [p, smean]\n",
    "\n",
    "    for s, sxx in zip(suggesteds, sx):\n",
    "        if s == selected:\n",
    "            px = [sv, smean]\n",
    "        else:\n",
    "            sxx = np.array(sxx)[0]\n",
    "            nx = [sxx, smean]\n",
    "            nxs.append(nx)\n",
    "        \n",
    "    if px == []:\n",
    "        continue\n",
    "\n",
    "    for nx in nxs:\n",
    "        anchor_test.append(ax)\n",
    "        positive_test.append(px)\n",
    "        negative_test.append(nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import Model, metrics, layers\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import AUC, BinaryAccuracy\n",
    "from keras.layers import Dense, Dropout, Flatten, Concatenate, Input, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        return (ap_distance, an_distance)\n",
    "\n",
    "DP = 0.4\n",
    "\n",
    "suggested_tensor = Input(shape=[DIM])\n",
    "playing_tensor = Input(shape=[DIM])\n",
    "\n",
    "\n",
    "sg_dense = Dense(128, activation='relu')(suggested_tensor)\n",
    "sg_dense = Dropout(DP)(sg_dense)\n",
    "sg = Flatten()(sg_dense)\n",
    "\n",
    "pl_dense = Dense(128, activation='relu')(playing_tensor)\n",
    "pl_dense = Dropout(DP)(pl_dense)\n",
    "pl = Flatten()(pl_dense)\n",
    "\n",
    "merged = Concatenate()([sg, pl])\n",
    "\n",
    "d = Dense(128, activation='relu')(merged)\n",
    "d = Dropout(DP)(d)\n",
    "flatten = Flatten()(d)\n",
    "output = Dense(64, activation=\"relu\")(flatten)\n",
    "# dense1 = Dropout(DP)(dense1)\n",
    "# dense1 = BatchNormalization()(dense1)\n",
    "# output = Dense(64, activation=\"relu\")(dense1)\n",
    "\n",
    "embedding = Model(inputs=[playing_tensor, suggested_tensor], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(Model):\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super().__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_playing = Input(shape=[DIM], name='anchor_p')\n",
    "anchor_suggested = Input(shape=[DIM], name='anchor_s')\n",
    "\n",
    "positive_selected = Input(shape=[DIM], name='positive_p')\n",
    "positive_suggested = Input(shape=[DIM], name='positive_s')\n",
    "\n",
    "negative_selected = Input(shape=[DIM], name='negative_p')\n",
    "negative_suggested = Input(shape=[DIM], name='negative_s')\n",
    "\n",
    "distances = DistanceLayer()(\n",
    "    embedding([anchor_playing, anchor_suggested]),\n",
    "    embedding([positive_selected, positive_suggested]),\n",
    "    embedding([negative_selected, negative_suggested]),\n",
    ")\n",
    "\n",
    "siamese_network = Model(\n",
    "    inputs = [anchor_playing, anchor_suggested, positive_selected, positive_suggested, negative_selected, negative_suggested], outputs=distances\n",
    ")\n",
    "\n",
    "siamese_model = SiameseModel(siamese_network)\n",
    "siamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ap, As, Pp, Ps, Np, Ns = [], [], [], [], [], []\n",
    "for ax, px, nx in zip(anchor, positive, negative):\n",
    "    Ap.append(ax[0])\n",
    "    As.append(ax[1])\n",
    "\n",
    "    Pp.append(px[0])\n",
    "    Ps.append(px[1])\n",
    "\n",
    "    Np.append(nx[0])\n",
    "    Ns.append(nx[1])\n",
    "\n",
    "Ap = np.array(Ap)\n",
    "As = np.array(As)\n",
    "\n",
    "Pp = np.array(Pp)\n",
    "Ps = np.array(Ps)\n",
    "\n",
    "Np = np.array(Np)\n",
    "Ns = np.array(Ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "1912/1912 [==============================] - 11s 6ms/step - loss: 0.0704 - val_loss: 0.0959\n",
      "Epoch 2/32\n",
      "1912/1912 [==============================] - 11s 6ms/step - loss: 0.0471 - val_loss: 0.1031\n",
      "Epoch 3/32\n",
      "1912/1912 [==============================] - 11s 6ms/step - loss: 0.0401 - val_loss: 0.1041\n",
      "Epoch 4/32\n",
      "1912/1912 [==============================] - 10s 5ms/step - loss: 0.0362 - val_loss: 0.1021\n",
      "Epoch 5/32\n",
      "1912/1912 [==============================] - 10s 5ms/step - loss: 0.0330 - val_loss: 0.1450\n",
      "Epoch 6/32\n",
      "1912/1912 [==============================] - 11s 6ms/step - loss: 0.0309 - val_loss: 0.1310\n",
      "Epoch 7/32\n",
      "1912/1912 [==============================] - 11s 6ms/step - loss: 0.0285 - val_loss: 0.1320\n",
      "Epoch 8/32\n",
      "1912/1912 [==============================] - 10s 5ms/step - loss: 0.0272 - val_loss: 0.1258\n",
      "Epoch 9/32\n",
      "1912/1912 [==============================] - 10s 5ms/step - loss: 0.0257 - val_loss: 0.1375\n",
      "Epoch 10/32\n",
      "1912/1912 [==============================] - 11s 6ms/step - loss: 0.0255 - val_loss: 0.1499\n",
      "Epoch 11/32\n",
      "1912/1912 [==============================] - 11s 6ms/step - loss: 0.0244 - val_loss: 0.1446\n",
      "Epoch 12/32\n",
      "1912/1912 [==============================] - 11s 6ms/step - loss: 0.0227 - val_loss: 0.1392\n",
      "Epoch 13/32\n",
      "1912/1912 [==============================] - 11s 6ms/step - loss: 0.0216 - val_loss: 0.1508\n",
      "Epoch 14/32\n",
      "1778/1912 [==========================>...] - ETA: 0s - loss: 0.0211"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m siamese_model\u001b[39m.\u001b[39;49mfit([Ap, As, Pp, Ps, Np, Ns], batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:149\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m    148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39m_call_flat(\n\u001b[0;32m--> 149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1534\u001b[0m, in \u001b[0;36mConcreteFunction.captured_inputs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   1529\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcaptured_inputs\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1530\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns external Tensors captured by this function.\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \n\u001b[1;32m   1532\u001b[0m \u001b[39m  self.__call__(*args) passes `args + self.captured_inputs` to the function.\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1534\u001b[0m   \u001b[39mreturn\u001b[39;00m nest\u001b[39m.\u001b[39;49mflatten(\n\u001b[1;32m   1535\u001b[0m       [x() \u001b[39mif\u001b[39;49;00m \u001b[39mcallable\u001b[39;49m(x) \u001b[39melse\u001b[39;49;00m x \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_captured_inputs],\n\u001b[1;32m   1536\u001b[0m       expand_composites\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/tensorflow/python/util/nest.py:289\u001b[0m, in \u001b[0;36mflatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnest.flatten\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflatten\u001b[39m(structure, expand_composites\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    196\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns a flat list from a given structure.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \n\u001b[1;32m    198\u001b[0m \u001b[39m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m    TypeError: The nest is or contains a dict with non-sortable keys.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m   \u001b[39mreturn\u001b[39;00m nest_util\u001b[39m.\u001b[39;49mflatten(\n\u001b[1;32m    290\u001b[0m       nest_util\u001b[39m.\u001b[39;49mModality\u001b[39m.\u001b[39;49mCORE, structure, expand_composites\n\u001b[1;32m    291\u001b[0m   )\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/tensorflow/python/util/nest_util.py:701\u001b[0m, in \u001b[0;36mflatten\u001b[0;34m(modality, structure, expand_composites)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Flattens a nested structure.\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \n\u001b[1;32m    607\u001b[0m \u001b[39m- For Modality.CORE: refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[39m  TypeError: The nest is or contains a dict with non-sortable keys.\u001b[39;00m\n\u001b[1;32m    699\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    700\u001b[0m \u001b[39mif\u001b[39;00m modality \u001b[39m==\u001b[39m Modality\u001b[39m.\u001b[39mCORE:\n\u001b[0;32m--> 701\u001b[0m   \u001b[39mreturn\u001b[39;00m _tf_core_flatten(structure, expand_composites)\n\u001b[1;32m    702\u001b[0m \u001b[39melif\u001b[39;00m modality \u001b[39m==\u001b[39m Modality\u001b[39m.\u001b[39mDATA:\n\u001b[1;32m    703\u001b[0m   \u001b[39mreturn\u001b[39;00m _tf_data_flatten(structure)\n",
      "File \u001b[0;32m~/.conda/envs/ytbase/lib/python3.9/site-packages/tensorflow/python/util/nest_util.py:715\u001b[0m, in \u001b[0;36m_tf_core_flatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    713\u001b[0m   \u001b[39mreturn\u001b[39;00m [\u001b[39mNone\u001b[39;00m]\n\u001b[1;32m    714\u001b[0m expand_composites \u001b[39m=\u001b[39m \u001b[39mbool\u001b[39m(expand_composites)\n\u001b[0;32m--> 715\u001b[0m \u001b[39mreturn\u001b[39;00m _pywrap_utils\u001b[39m.\u001b[39;49mFlatten(structure, expand_composites)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "siamese_model.fit([Ap, As, Pp, Ps, Np, Ns], batch_size=64, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ap_t, As_t, Pp_t, Ps_t, Np_t, Ns_t = [], [], [], [], [], []\n",
    "for ax, px, nx in zip(anchor_test, positive_test, negative_test):\n",
    "    Ap_t.append(ax[0])\n",
    "    As_t.append(ax[1])\n",
    "\n",
    "    Pp_t.append(px[0])\n",
    "    Ps_t.append(px[1])\n",
    "\n",
    "    Np_t.append(nx[0])\n",
    "    Ns_t.append(nx[1])\n",
    "\n",
    "Ap_t = np.array(Ap_t)\n",
    "As_t = np.array(As_t)\n",
    "\n",
    "Pp_t = np.array(Pp_t)\n",
    "Ps_t = np.array(Ps_t)\n",
    "\n",
    "Np_t = np.array(Np_t)\n",
    "Ns_t = np.array(Ns_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/598 [..............................] - ETA: 12s - loss: 0.0857"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598/598 [==============================] - 1s 1ms/step - loss: 0.1194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11936970055103302"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_model.evaluate([Ap_t, As_t, Pp_t, Ps_t, Np_t, Ns_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = embedding([Ap_t, As_t]),\n",
    "a = np.array(a)[0]\n",
    "\n",
    "p = embedding([Pp_t, Ps_t]),\n",
    "p = np.array(p)[0]\n",
    "\n",
    "n = embedding([Np_t, Ns_t]),\n",
    "n = np.array(n)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tp = 0\n",
    "tn = 0\n",
    "total = 0\n",
    "\n",
    "for anc, pos, neg in zip(a, p, n):\n",
    "\n",
    "    pv = cosine_similarity(anc.reshape(1, -1), pos.reshape(1, -1)).flatten()[0]\n",
    "    nv = cosine_similarity(anc.reshape(1, -1), neg.reshape(1, -1)).flatten()[0]\n",
    "\n",
    "    if pv > nv:\n",
    "        tp += 1\n",
    "    else:\n",
    "        tn += 1\n",
    "\n",
    "    total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9095140959255191"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(tn/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9969941"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ytbase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
