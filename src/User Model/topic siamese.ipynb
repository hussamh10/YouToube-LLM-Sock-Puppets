{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-15 09:21:58.174379: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-15 09:21:58.691058: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "watches = pd.read_pickle('../../data/user-level-data')\n",
    "videos = pd.read_pickle('../../data/videos_raw_metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "watches_df = dict()\n",
    "\n",
    "def convert(watch):\n",
    "    d = dict()\n",
    "    d['playing'] = watch['playing']['id']\n",
    "    suggested = []\n",
    "    for s in watch['suggested']:\n",
    "        if s != None:\n",
    "            suggested.append(s['id'])\n",
    "    d['suggested'] = suggested\n",
    "    d['selected'] = watch['selected']\n",
    "    return d\n",
    "\n",
    "for user in watches:\n",
    "    watches_df[user] = []\n",
    "    for watch in watches[user]:\n",
    "        if watch['playing'] == None:\n",
    "            continue\n",
    "        watch = convert(watch)\n",
    "        watches_df[user].append(watch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = dict()\n",
    "users = list(watches_df.keys())\n",
    "data = dict()\n",
    "\n",
    "for user in users:\n",
    "    if len(watches_df[user]) < 10:\n",
    "        continue\n",
    "    history[user] = watches_df[user][:10]\n",
    "\n",
    "    for watch in watches_df[user][10:]:\n",
    "        if watch['selected'] != None:\n",
    "            if user not in data:\n",
    "                data[user] = []\n",
    "            data[user].append(watch)\n",
    "\n",
    "for user in list(history.keys()):\n",
    "    if user not in data:\n",
    "        history.pop(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in history:\n",
    "    history[user] = [s['playing'] for s in history[user]]\n",
    "    history[user] = [get_topic_vector(s) for s in history[user]]\n",
    "    history[user] = np.array(history[user]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "all_topics = []\n",
    "all_tags = []\n",
    "\n",
    "for video in videos:\n",
    "    video = videos[video]\n",
    "    \n",
    "    try:\n",
    "        topics = video['topicDetails']['topicCategories']\n",
    "        topics = [topic.split('/')[-1].lower() for topic in topics]\n",
    "    except:\n",
    "        topics = []\n",
    "\n",
    "    all_topics += topics\n",
    "    \n",
    "    try:\n",
    "        tags = video['snippet']['tags']\n",
    "        tags = [tag.lower() for tag in tags]\n",
    "    except:\n",
    "        tags = []\n",
    "\n",
    "    all_tags += tags\n",
    "\n",
    "all_topics = list(set(all_topics))\n",
    "\n",
    "atc = Counter(all_tags)\n",
    "atc = pd.DataFrame.from_dict(atc, 'index', columns=['count'])\n",
    "atc = atc.sort_values(['count'], ascending=False).head(5000)\n",
    "all_tags = list(atc.index)\n",
    "\n",
    "def get_one_hot_vector(topics, tags):\n",
    "    oh_topics = np.zeros(len(all_topics))\n",
    "    for topic in topics:\n",
    "        oh_topics[all_topics.index(topic)] = 1\n",
    "        \n",
    "    oh_tags = np.zeros(len(all_tags))\n",
    "    for tag in tags:\n",
    "        if tag in all_tags:\n",
    "            oh_tags[all_tags.index(tag)] = 1\n",
    "        \n",
    "    oh = np.concatenate([oh_topics , oh_tags])\n",
    "    \n",
    "    return oh\n",
    "\n",
    "\n",
    "def get_topic_vector(video):\n",
    "    video = videos[video]\n",
    "    try:\n",
    "        topics = video['topicDetails']['topicCategories']\n",
    "        topics = [topic.split('/')[-1].lower() for topic in topics]\n",
    "    except:\n",
    "        topics = []\n",
    "\n",
    "    try:\n",
    "        tags = video['snippet']['tags']\n",
    "        tags = [tag.lower() for tag in tags]\n",
    "    except:\n",
    "        tags = []\n",
    "\n",
    "    return get_one_hot_vector(topics, tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceae281a89e541bfb1a830b86847983d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AX = []\n",
    "PX = []\n",
    "NX = []\n",
    "Y = []\n",
    "topic_vectors = dict()\n",
    "\n",
    "for user in tqdm(data):\n",
    "    user_history = history[user]\n",
    "    user_history = np.concatenate([user_history, [0]])\n",
    "    for watch in data[user]:\n",
    "        \n",
    "        playing = watch['playing']\n",
    "        selected = watch['selected']\n",
    "        upnext = watch['suggested'][:15]\n",
    "\n",
    "        if playing not in videos or selected not in videos or any([s not in videos for s in watch['suggested']]):\n",
    "            continue\n",
    "        if len(watch['suggested']) < 15:\n",
    "            continue\n",
    "\n",
    "\n",
    "        playing = get_topic_vector(watch['playing'])\n",
    "        playing = np.concatenate([playing, [0]])\n",
    "\n",
    "        suggesteds = []\n",
    "        \n",
    "        for i, s in enumerate(upnext):\n",
    "            if s not in topic_vectors:\n",
    "                topic_vectors[s] = get_topic_vector(s)\n",
    "            sv = topic_vectors[s]\n",
    "            sv = np.concatenate([sv, [i]])\n",
    "            suggesteds.append(sv)\n",
    "        \n",
    "        suggesteds = np.array(suggesteds)\n",
    "        suggesteds = np.mean(suggesteds, axis=0)\n",
    "\n",
    "        nxs = []\n",
    "        px = []\n",
    "        ax = [playing, suggesteds, user_history]\n",
    "\n",
    "        for i, s in enumerate(upnext):\n",
    "            selected = watch['selected']\n",
    "\n",
    "            sv = topic_vectors[s]\n",
    "            sv = np.concatenate([sv, [i]])\n",
    "\n",
    "            if s == selected:\n",
    "                px = [sv, suggesteds, user_history]\n",
    "            else:\n",
    "                nx = [sv, suggesteds, user_history]\n",
    "                nxs.append(nx)\n",
    "\n",
    "\n",
    "        if px == []:\n",
    "            continue\n",
    "        for nx in nxs:\n",
    "            AX.append(ax)\n",
    "            PX.append(px)\n",
    "            NX.append(nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "AX_train, AX_test, PX_train, PX_test, NX_train, NX_test = train_test_split(AX, PX, NX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import Model, metrics, layers\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import AUC, BinaryAccuracy\n",
    "from keras.layers import Dense, Dropout, Flatten, Concatenate, Input, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 5063"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-15 09:25:00.511556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46692 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-08-15 09:25:00.512415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46692 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "class DistanceLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        return (ap_distance, an_distance)\n",
    "\n",
    "DP = 0.3\n",
    "\n",
    "suggested_tensor = Input(shape=[DIM])\n",
    "playing_tensor = Input(shape=[DIM])\n",
    "history_tensor = Input(shape=[DIM])\n",
    "\n",
    "\n",
    "sg_dense = Dense(1024, activation='relu')(suggested_tensor)\n",
    "sg_dense = Dropout(DP)(sg_dense)\n",
    "sg = Flatten()(sg_dense)\n",
    "\n",
    "pl_dense = Dense(1024, activation='relu')(playing_tensor)\n",
    "pl_dense = Dropout(DP)(pl_dense)\n",
    "pl = Flatten()(pl_dense)\n",
    "\n",
    "hl_dense = Dense(1024, activation='relu')(history_tensor)\n",
    "hl_dense = Dropout(DP)(hl_dense)\n",
    "hl = Flatten()(hl_dense)\n",
    "\n",
    "merged = Concatenate()([sg, pl, hl])\n",
    "\n",
    "d = Dense(2056, activation='relu')(merged)\n",
    "d = Dropout(DP)(d)\n",
    "d = Dense(1024, activation='relu')(d)\n",
    "d = Dropout(DP)(d)\n",
    "flatten = Flatten()(d)\n",
    "dense1 = Dense(1024, activation=\"relu\")(flatten)\n",
    "dense1 = Dropout(DP)(dense1)\n",
    "dense1 = BatchNormalization()(dense1)\n",
    "dense2 = Dense(256, activation=\"relu\")(dense1)\n",
    "dense2 = Dropout(DP)(dense2)\n",
    "dense2 = BatchNormalization()(dense2)\n",
    "output = Dense(256)(dense2)\n",
    "\n",
    "embedding = Model(inputs=[playing_tensor, suggested_tensor, history_tensor], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(Model):\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super().__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_playing = Input(shape=[DIM], name='anchor_p')\n",
    "anchor_suggested = Input(shape=[DIM], name='anchor_s')\n",
    "anchor_history = Input(shape=[DIM], name='anchor_h')\n",
    "\n",
    "positive_selected = Input(shape=[DIM], name='positive_p')\n",
    "positive_suggested = Input(shape=[DIM], name='positive_s')\n",
    "positive_history = Input(shape=[DIM], name='positive_h')\n",
    "\n",
    "negative_selected = Input(shape=[DIM], name='negative_p')\n",
    "negative_suggested = Input(shape=[DIM], name='negative_s')\n",
    "negative_history = Input(shape=[DIM], name='negative_h')\n",
    "\n",
    "distances = DistanceLayer()(\n",
    "    embedding([anchor_playing, anchor_suggested, anchor_history]),\n",
    "    embedding([positive_selected, positive_suggested, positive_history]),\n",
    "    embedding([negative_selected, negative_suggested, negative_history]),\n",
    ")\n",
    "\n",
    "\n",
    "# distances = DistanceLayer()(\n",
    "#     embedding(anchor),\n",
    "#     embedding(positive),\n",
    "#     embedding(negative),\n",
    "# )\n",
    "\n",
    "siamese_network = Model(\n",
    "    inputs = [anchor_playing, anchor_suggested, anchor_history, positive_selected, positive_suggested, positive_history, negative_selected, negative_suggested, negative_history], outputs=distances\n",
    ")\n",
    "\n",
    "siamese_model = SiameseModel(siamese_network)\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "siamese_model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ap, As, Ah, Pp, Ps, Ph, Np, Ns, Nh = [], [], [], [], [], [], [], [], []\n",
    "for ax, px, nx in zip(AX_train, PX_train, NX_train):\n",
    "    Ap.append(ax[0])\n",
    "    As.append(ax[1])\n",
    "    Ah.append(ax[2])\n",
    "\n",
    "    Pp.append(px[0])\n",
    "    Ps.append(px[1])\n",
    "    Ph.append(px[2])\n",
    "\n",
    "    Np.append(nx[0])\n",
    "    Ns.append(nx[1])\n",
    "    Nh.append(nx[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ap = np.array(Ap)\n",
    "As = np.array(As)\n",
    "Ah = np.array(Ah)\n",
    "\n",
    "Pp = np.array(Pp)\n",
    "Ps = np.array(Ps)\n",
    "Ph = np.array(Ph)\n",
    "\n",
    "Np = np.array(Np)\n",
    "Ns = np.array(Ns)\n",
    "Nh = np.array(Nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-15 09:25:50.699928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-08-15 09:25:50.704855: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f6e0c49ead0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-15 09:25:50.704871: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2023-08-15 09:25:50.704877: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2023-08-15 09:25:50.710153: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-08-15 09:25:50.818192: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-08-15 09:25:50.920367: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3908/3908 [==============================] - 52s 13ms/step - loss: 0.2390 - val_loss: 0.2016\n",
      "Epoch 2/100\n",
      "3908/3908 [==============================] - 40s 10ms/step - loss: 0.1586 - val_loss: 0.1593\n",
      "Epoch 3/100\n",
      "3908/3908 [==============================] - 40s 10ms/step - loss: 0.1224 - val_loss: 0.1388\n",
      "Epoch 4/100\n",
      "3908/3908 [==============================] - 40s 10ms/step - loss: 0.1025 - val_loss: 0.1294\n",
      "Epoch 5/100\n",
      "3908/3908 [==============================] - 42s 11ms/step - loss: 0.0872 - val_loss: 0.1399\n",
      "Epoch 6/100\n",
      "3908/3908 [==============================] - 42s 11ms/step - loss: 0.0756 - val_loss: 0.1295\n",
      "Epoch 7/100\n",
      "3908/3908 [==============================] - 42s 11ms/step - loss: 0.0673 - val_loss: 0.1228\n",
      "Epoch 8/100\n",
      "3908/3908 [==============================] - 40s 10ms/step - loss: 0.0599 - val_loss: 0.1221\n",
      "Epoch 9/100\n",
      "3908/3908 [==============================] - 42s 11ms/step - loss: 0.0532 - val_loss: 0.1236\n",
      "Epoch 10/100\n",
      "3908/3908 [==============================] - 39s 10ms/step - loss: 0.0485 - val_loss: 0.1502\n",
      "Epoch 11/100\n",
      "3908/3908 [==============================] - 43s 11ms/step - loss: 0.0450 - val_loss: 0.1314\n",
      "Epoch 12/100\n",
      "3908/3908 [==============================] - 40s 10ms/step - loss: 0.0397 - val_loss: 0.1382\n",
      "Epoch 13/100\n",
      "3908/3908 [==============================] - 40s 10ms/step - loss: 0.0361 - val_loss: 0.1212\n",
      "Epoch 14/100\n",
      "3908/3908 [==============================] - 41s 11ms/step - loss: 0.0331 - val_loss: 0.1283\n",
      "Epoch 15/100\n",
      "3908/3908 [==============================] - 41s 10ms/step - loss: 0.0311 - val_loss: 0.1432\n",
      "Epoch 16/100\n",
      "3908/3908 [==============================] - 40s 10ms/step - loss: 0.0289 - val_loss: 0.1270\n",
      "Epoch 17/100\n",
      "3908/3908 [==============================] - 41s 10ms/step - loss: 0.0272 - val_loss: 0.1200\n",
      "Epoch 18/100\n",
      "3908/3908 [==============================] - 43s 11ms/step - loss: 0.0250 - val_loss: 0.1231\n",
      "Epoch 19/100\n",
      "3908/3908 [==============================] - 40s 10ms/step - loss: 0.0239 - val_loss: 0.1466\n",
      "Epoch 20/100\n",
      "3908/3908 [==============================] - 42s 11ms/step - loss: 0.0225 - val_loss: 0.1236\n",
      "Epoch 21/100\n",
      "3908/3908 [==============================] - 42s 11ms/step - loss: 0.0221 - val_loss: 0.1101\n",
      "Epoch 22/100\n",
      "3908/3908 [==============================] - 42s 11ms/step - loss: 0.0204 - val_loss: 0.1312\n",
      "Epoch 23/100\n",
      "3908/3908 [==============================] - 40s 10ms/step - loss: 0.0199 - val_loss: 0.1255\n",
      "Epoch 24/100\n",
      "3908/3908 [==============================] - 40s 10ms/step - loss: 0.0189 - val_loss: 0.1395\n",
      "Epoch 25/100\n",
      "3908/3908 [==============================] - 40s 10ms/step - loss: 0.0182 - val_loss: 0.1205\n",
      "Epoch 26/100\n",
      "3908/3908 [==============================] - 42s 11ms/step - loss: 0.0175 - val_loss: 0.1455\n",
      "Epoch 27/100\n",
      "3908/3908 [==============================] - 42s 11ms/step - loss: 0.0170 - val_loss: 0.1232\n",
      "Epoch 28/100\n",
      "3908/3908 [==============================] - 42s 11ms/step - loss: 0.0154 - val_loss: 0.1397\n",
      "Epoch 29/100\n",
      "3908/3908 [==============================] - 41s 11ms/step - loss: 0.0165 - val_loss: 0.1416\n",
      "Epoch 30/100\n",
      "3908/3908 [==============================] - 42s 11ms/step - loss: 0.0153 - val_loss: 0.1355\n",
      "Epoch 31/100\n",
      "3908/3908 [==============================] - 42s 11ms/step - loss: 0.0147 - val_loss: 0.1327\n",
      "Epoch 32/100\n",
      "3908/3908 [==============================] - 42s 11ms/step - loss: 0.0145 - val_loss: 0.1342\n",
      "Epoch 33/100\n",
      "3908/3908 [==============================] - 42s 11ms/step - loss: 0.0144 - val_loss: 0.1384\n",
      "Epoch 34/100\n",
      "3908/3908 [==============================] - 43s 11ms/step - loss: 0.0134 - val_loss: 0.1539\n",
      "Epoch 35/100\n",
      "3908/3908 [==============================] - 44s 11ms/step - loss: 0.0133 - val_loss: 0.1416\n",
      "Epoch 36/100\n",
      "3908/3908 [==============================] - 43s 11ms/step - loss: 0.0131 - val_loss: 0.1339\n",
      "Epoch 37/100\n",
      "3908/3908 [==============================] - 43s 11ms/step - loss: 0.0128 - val_loss: 0.1420\n",
      "Epoch 38/100\n",
      "3908/3908 [==============================] - 44s 11ms/step - loss: 0.0128 - val_loss: 0.1339\n",
      "Epoch 39/100\n",
      "3908/3908 [==============================] - 44s 11ms/step - loss: 0.0120 - val_loss: 0.1625\n",
      "Epoch 40/100\n",
      "3908/3908 [==============================] - 44s 11ms/step - loss: 0.0115 - val_loss: 0.1380\n",
      "Epoch 41/100\n",
      "3908/3908 [==============================] - 43s 11ms/step - loss: 0.0122 - val_loss: 0.1315\n",
      "Epoch 42/100\n",
      "3908/3908 [==============================] - 44s 11ms/step - loss: 0.0119 - val_loss: 0.1496\n",
      "Epoch 43/100\n",
      "3908/3908 [==============================] - 43s 11ms/step - loss: 0.0112 - val_loss: 0.1344\n",
      "Epoch 44/100\n",
      "3908/3908 [==============================] - 43s 11ms/step - loss: 0.0110 - val_loss: 0.1509\n",
      "Epoch 45/100\n",
      "3908/3908 [==============================] - 43s 11ms/step - loss: 0.0119 - val_loss: 0.1551\n",
      "Epoch 46/100\n",
      "3908/3908 [==============================] - 43s 11ms/step - loss: 0.0107 - val_loss: 0.1583\n",
      "Epoch 47/100\n",
      "2555/3908 [==================>...........] - ETA: 13s - loss: 0.0108"
     ]
    }
   ],
   "source": [
    "siamese_model.fit([Ap, As, Ah, Pp, Ps, Ph, Np, Ns, Nh], batch_size=32, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ytbase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
