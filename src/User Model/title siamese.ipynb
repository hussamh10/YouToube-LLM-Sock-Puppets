{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 12:22:34.375991: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-17 12:22:34.988100: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 12:22:56.919060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46692 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-08-17 12:22:56.919782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46692 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('../../data/datasets/raw-video-level-watches')\n",
    "videos = pd.read_pickle('../../data/videos_raw_metadata')\n",
    "title_embeddings = pd.read_pickle('../../data/embeddings/title-autoencoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(vec, element, length=15):\n",
    "    #  a vector to length and keep the element\n",
    "    if len(vec) > length:\n",
    "        vec = vec[:length]\n",
    "    if element not in vec:\n",
    "        vec = vec[:-1] + [element]\n",
    "\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = []\n",
    "positive = []\n",
    "negative = []\n",
    "\n",
    "for watch in df:\n",
    "    playing = watch['playing']\n",
    "    selected = watch['selected']\n",
    "    suggesteds = watch['upnext']\n",
    "\n",
    "    suggesteds = trim(suggesteds, selected)\n",
    "    \n",
    "    if playing not in videos or selected not in videos or any([s not in videos for s in suggesteds]):\n",
    "        continue\n",
    "    \n",
    "    p = np.array(title_embeddings[videos[playing]['snippet']['title']])[0]\n",
    "    sv = np.array(title_embeddings[videos[selected]['snippet']['title']])[0]\n",
    "    sx = [title_embeddings[videos[s]['snippet']['title']] for s in suggesteds]\n",
    "    smean = np.mean(np.array(sx), axis=0)[0]\n",
    "    nxs = []\n",
    "    px = []\n",
    "    ax = [p, smean]\n",
    "\n",
    "    for s, sxx in zip(suggesteds, sx):\n",
    "        if s == selected:\n",
    "            px = [sv, smean]\n",
    "        else:\n",
    "            sxx = np.array(sxx)[0]\n",
    "            nx = [sxx, smean]\n",
    "            nxs.append(nx)\n",
    "        \n",
    "    if px == []:\n",
    "        continue\n",
    "\n",
    "    for nx in nxs:\n",
    "        anchor.append(ax)\n",
    "        positive.append(px)\n",
    "        negative.append(nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "AX_train, AX_test, PX_train, PX_test, NX_train, NX_test = train_test_split(anchor, positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import Model, metrics, layers\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import AUC, BinaryAccuracy\n",
    "from keras.layers import Dense, Dropout, Flatten, Concatenate, Input, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        return (ap_distance, an_distance)\n",
    "\n",
    "DP = 0.3\n",
    "\n",
    "suggested_tensor = Input(shape=[DIM])\n",
    "playing_tensor = Input(shape=[DIM])\n",
    "\n",
    "\n",
    "sg_dense = Dense(128, activation='relu')(suggested_tensor)\n",
    "sg_dense = Dropout(DP)(sg_dense)\n",
    "sg = Flatten()(sg_dense)\n",
    "\n",
    "pl_dense = Dense(128, activation='relu')(playing_tensor)\n",
    "pl_dense = Dropout(DP)(pl_dense)\n",
    "pl = Flatten()(pl_dense)\n",
    "\n",
    "merged = Concatenate()([sg, pl])\n",
    "\n",
    "d = Dense(128, activation='relu')(merged)\n",
    "d = Dropout(DP)(d)\n",
    "flatten = Flatten()(d)\n",
    "dense1 = Dense(128, activation=\"relu\")(flatten)\n",
    "dense1 = Dropout(DP)(dense1)\n",
    "dense1 = BatchNormalization()(dense1)\n",
    "output = Dense(128, activation=\"relu\")(dense1)\n",
    "\n",
    "embedding = Model(inputs=[playing_tensor, suggested_tensor], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(Model):\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super().__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_playing = Input(shape=[DIM], name='anchor_p')\n",
    "anchor_suggested = Input(shape=[DIM], name='anchor_s')\n",
    "anchor_history = Input(shape=[DIM], name='anchor_h')\n",
    "\n",
    "positive_selected = Input(shape=[DIM], name='positive_p')\n",
    "positive_suggested = Input(shape=[DIM], name='positive_s')\n",
    "positive_history = Input(shape=[DIM], name='positive_h')\n",
    "\n",
    "negative_selected = Input(shape=[DIM], name='negative_p')\n",
    "negative_suggested = Input(shape=[DIM], name='negative_s')\n",
    "negative_history = Input(shape=[DIM], name='negative_h')\n",
    "\n",
    "distances = DistanceLayer()(\n",
    "    embedding([anchor_playing, anchor_suggested]),\n",
    "    embedding([positive_selected, positive_suggested]),\n",
    "    embedding([negative_selected, negative_suggested]),\n",
    ")\n",
    "\n",
    "\n",
    "# distances = DistanceLayer()(\n",
    "#     embedding(anchor),\n",
    "#     embedding(positive),\n",
    "#     embedding(negative),\n",
    "# )\n",
    "\n",
    "siamese_network = Model(\n",
    "    inputs = [anchor_playing, anchor_suggested, positive_selected, positive_suggested, negative_selected, negative_suggested], outputs=distances\n",
    ")\n",
    "\n",
    "siamese_model = SiameseModel(siamese_network)\n",
    "siamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ap, As, Pp, Ps, Np, Ns = [], [], [], [], [], []\n",
    "for ax, px, nx in zip(AX_train, PX_train, NX_train):\n",
    "    Ap.append(ax[0])\n",
    "    As.append(ax[1])\n",
    "\n",
    "    Pp.append(px[0])\n",
    "    Ps.append(px[1])\n",
    "\n",
    "    Np.append(nx[0])\n",
    "    Ns.append(nx[1])\n",
    "\n",
    "Ap = np.array(Ap)\n",
    "As = np.array(As)\n",
    "\n",
    "Pp = np.array(Pp)\n",
    "Ps = np.array(Ps)\n",
    "\n",
    "Np = np.array(Np)\n",
    "Ns = np.array(Ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 733/1434 [==============>...............] - ETA: 2s - loss: 0.4202"
     ]
    }
   ],
   "source": [
    "siamese_model.fit([Ap, As, Pp, Ps, Np, Ns], batch_size=32, epochs=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ap_t, As_t, Pp_t, Ps_t, Np_t, Ns_t = [], [], [], [], [], []\n",
    "for ax, px, nx in zip(AX_test, PX_test, NX_test):\n",
    "    Ap_t.append(ax[0])\n",
    "    As_t.append(ax[1])\n",
    "\n",
    "    Pp_t.append(px[0])\n",
    "    Ps_t.append(px[1])\n",
    "\n",
    "    Np_t.append(nx[0])\n",
    "    Ns_t.append(nx[1])\n",
    "\n",
    "Ap_t = np.array(Ap_t)\n",
    "As_t = np.array(As_t)\n",
    "\n",
    "Pp_t = np.array(Pp_t)\n",
    "Ps_t = np.array(Ps_t)\n",
    "\n",
    "Np_t = np.array(Np_t)\n",
    "Ns_t = np.array(Ns_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/598 [..............................] - ETA: 12s - loss: 0.0857"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598/598 [==============================] - 1s 1ms/step - loss: 0.1194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11936970055103302"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_model.evaluate([Ap_t, As_t, Pp_t, Ps_t, Np_t, Ns_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = embedding([Ap_t, As_t]),\n",
    "a = np.array(a)[0]\n",
    "\n",
    "p = embedding([Pp_t, Ps_t]),\n",
    "p = np.array(p)[0]\n",
    "\n",
    "n = embedding([Np_t, Ns_t]),\n",
    "n = np.array(n)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tp = 0\n",
    "tn = 0\n",
    "total = 0\n",
    "\n",
    "for anc, pos, neg in zip(a, p, n):\n",
    "\n",
    "    pv = cosine_similarity(anc.reshape(1, -1), pos.reshape(1, -1)).flatten()[0]\n",
    "    nv = cosine_similarity(anc.reshape(1, -1), neg.reshape(1, -1)).flatten()[0]\n",
    "\n",
    "    if pv > nv:\n",
    "        tp += 1\n",
    "    else:\n",
    "        tn += 1\n",
    "\n",
    "    total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9095140959255191"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(tn/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9969941"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ytbase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
